{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46c23bc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f837acc5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from DataAquisition import DataAquisition\n",
    "from FeatureSelection import FeatureSelector\n",
    "from VARModel import VARModel\n",
    "from CombineSheet import CombineSheet\n",
    "import os\n",
    "from deepseeklm import DeepSeekLM\n",
    "import json\n",
    "import pandas as pd\n",
    "from DataAnalysis import DataAnalysis\n",
    "from Denormalizer import Denormalizer\n",
    "from Pix2StructAnalyzer import Pix2StructAnalyzer\n",
    "from FeatureSelection import FeatureSelector\n",
    "from Llama import LlamaInference\n",
    "from LSTM_analysis import LSTMAnalysis\n",
    "from LSTM_model import LSTMModel\n",
    "import os\n",
    "from EDA2 import EDA\n",
    "from DataCleaning1 import DataCleaner\n",
    "from DataTransformation1 import DataTransformer\n",
    "from corr_analysis1 import CorrelationAnalysis\n",
    "from modeling import Modeling\n",
    "from RegressionModels.NeuralNetworkRegression import NeuralNetworkRegression\n",
    "from RegressionModels.MultiLinearRegression import MultiLinearRegression\n",
    "from IPython.display import display\n",
    "from dynamic_format_final import DataScienceReport\n",
    "from Medflow_files.Medflow import Medflow\n",
    "import torch\n",
    "import sys\n",
    "auth_token = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df783e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "\n",
    "  \"KHI_MEDFLOW_DB_HOST\": \"10.0.0.244\",\n",
    "  \"KHI_MEDFLOW_DB_USER\": \"testuser\",\n",
    "  \"KHI_MEDFLOW_DB_PASS\": \"DoubleZero@987!\",\n",
    "  \"KHI_MEDFLOW_DB_DATABASE\": \"EPM2\" ,\n",
    "  \"KHI_MEDFLOW_DB_FILE_UPLOAD_USER\" : \"4332\",\n",
    "  \"KHI_MEDFLOW_DB_USER_MRN_BILLING\": \"ai_mrn_billing\",\n",
    "\n",
    "  \"KHI_MEDFLOW_DB_USER_MRN_IWMG\": \"ai_mrn_iwmg\",\n",
    "\n",
    "  \"KHI_MEDFLOW_DB_USER_MRN_MAILING\": \"ai_mrn_mailing\",\n",
    "\n",
    "  \"KHI_MEDFLOW_DB_USER_MRN_MAILING_1\": \"ai_mrn_mailing_1\",\n",
    "\n",
    "  \"KHI_MEDFLOW_DB_USER_MRN_FILENET\": \"ai_mrn_filenet\",\n",
    "\n",
    "  \"KHI_MEDFLOW_DB_USER_UPLOAD_BILLING\": \"ai_upload_billing\",\n",
    "\n",
    "  \"KHI_MEDFLOW_DB_USER_UPLOAD_IWMG\": \"ai_upload_iwmg\",\n",
    "\n",
    "  \"KHI_MEDFLOW_DB_USER_UPLOAD_MAILING\": \"ai_upload_mailing\",\n",
    "\n",
    "  \"KHI_MEDFLOW_DB_USER_UPLOAD_MAILING_1\": \"ai_upload_mailing_1\",\n",
    "\n",
    "  \"KHI_MEDFLOW_DB_USER_UPLOAD_FILENET\": \"ai_upload_filenet\",\n",
    "\n",
    "  \"KHI_MEDFLOW_DB_USER_TESTING\": \"testuser\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975b80aa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# def data_source():\n",
    "\n",
    "try:\n",
    "    source = int(input(\"Enter your Data source, write 1 for xls/csv and 2 for Db:\")) # xls Db\n",
    "    if source == 1:\n",
    "        source = 'xls'\n",
    "    else:\n",
    "        source = 'Db'\n",
    "\n",
    "    if source == 'xls':\n",
    "        folder_path = input(\"Enter the directory path to read files: \")\n",
    "        try:\n",
    "            combiner = CombineSheet(folder_path)\n",
    "            folder_path = combiner.combine_tables()\n",
    "            print(folder_path)\n",
    "        except FileNotFoundError:\n",
    "            print(\"The specified folder path does not exist.\")\n",
    "\n",
    "    elif source == 'Db':\n",
    "        try:\n",
    "            medflow = Medflow(config)\n",
    "            folder_path = medflow.process()\n",
    "            print(folder_path)\n",
    "        except FileNotFoundError:\n",
    "            print(\"The specified file path does not exist.\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637d1dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f42c0fc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def pipeline():\n",
    "    \n",
    "    ## Loading DeepSeek LLM\n",
    "#     deepseek_lm = DeepSeekLM() \n",
    "    \n",
    "    data_loader=None\n",
    "    df = None\n",
    "    \n",
    "    ## Input path\n",
    "#     folder_path = input(\"Enter the directory path to read files: \")\n",
    "    \n",
    "    try:\n",
    "        if os.path.exists(folder_path) and os.path.isdir(folder_path):\n",
    "            for file_name in os.listdir(folder_path):\n",
    "                decision_query = None\n",
    "                if file_name.endswith('.csv') and os.path.isfile(os.path.join(folder_path, file_name)):\n",
    "                    data_loader = DataAquisition(source, folder_path, file_name)\n",
    "                    df = data_loader.read_data()\n",
    "                    print(f\"\\n\\n{file_name}---------------------------------------------------\\n\")\n",
    "                    print(f\"First few rows of file {file_name}\")\n",
    "                    display(df.head())\n",
    "                    print(\"\\nfile_name: \", file_name)\n",
    "                          \n",
    "                    try:\n",
    "                        target_var = input(\"Please Enter Right Exact Target Column Name: \")\n",
    "                        problem_type, target_datatype, type_column, date_index = data_loader.analyze_problem_type(df, target_var)\n",
    "                        print(\"Current Identified problem_type:\", problem_type)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(\"We encountered an issue! Please input a name that matches those in the dataset.\")\n",
    "                        return\n",
    "\n",
    "                    print(\"\\nfile_name: \", file_name)\n",
    "\n",
    "                    ## Craete Directories\n",
    "                    data_loader.make_directories(problem_type, file_name)\n",
    "                    data_loader.move_dataset_to_knowledge(df)\n",
    "\n",
    "\n",
    "                    dependent_variable = target_var\n",
    "                    ## Data Cleaning\n",
    "                    cleaning_instance = DataCleaner(source, df, file_name, problem_type, type_column, dependent_variable)\n",
    "                    df1 = cleaning_instance.get_data()\n",
    "                    clean =df1.info()\n",
    "                    print(\"cleanining\",clean)\n",
    "\n",
    "\n",
    "                    dependent_variable = target_var\n",
    "                    ## EDA \n",
    "                    EDA_instance = EDA(source, df1, file_name ,problem_type, type_column, dependent_variable,date_index)\n",
    "                    df =  EDA_instance.get_data()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    dependent_variable = target_var\n",
    "                    ## Feature Selection\n",
    "                    corr_instance = CorrelationAnalysis(source, df1, file_name, problem_type, type_column, dependent_variable, corr_thres = 0.3)\n",
    "                    df1 = corr_instance.get_data()\n",
    "                    most_corr_df = corr_instance.get_most_corr_data()\n",
    "#                     print(\"most_corr_df: \", most_corr_df)\n",
    "#                     print(\"df1: \", df1)\n",
    "\n",
    "\n",
    "                    ## Data Transformation\n",
    "                    if problem_type.lower() == \"time series\":\n",
    "                        tansformation_instance = DataTransformer(source, df1, file_name, problem_type, type_column, dependent_variable,date_index)\n",
    "                    else:\n",
    "                        tansformation_instance = DataTransformer(source, df1, file_name, problem_type, type_column, dependent_variable,date_index)\n",
    "                    # df = tansformation_instance.get_data()\n",
    "                    transformed_df = tansformation_instance.get_transformed_data()\n",
    "                    \n",
    "\n",
    "\n",
    "                    if problem_type.lower() == \"time series\":\n",
    "                        transformed_df = tansformation_instance.get_transformed_data()\n",
    "                        filled_scaler = tansformation_instance.get_filled_scaler()\n",
    "                        filled_encoder = tansformation_instance.get_filled_encoder()\n",
    "                        numeric_columns_names = tansformation_instance.numeric_columns\n",
    "                    elif problem_type.lower() == \"categorical\":\n",
    "                        transformed_df = tansformation_instance.transformed_data\n",
    "                        transformed_df_y = tansformation_instance.transformed_data_y\n",
    "                        filled_scaler = tansformation_instance.get_filled_scaler()\n",
    "                        filled_encoder = tansformation_instance.get_filled_encoder()\n",
    "                        filled_encoder_y = tansformation_instance.label_encoder_y\n",
    "                        numeric_columns_names = tansformation_instance.numeric_columns\n",
    "                        categorical_columns_names = tansformation_instance.categorical_columns\n",
    "                    elif problem_type.lower() == \"numerical\":\n",
    "                        transformed_df = tansformation_instance.transformed_data\n",
    "                        transformed_df_y = tansformation_instance.transformed_data_y\n",
    "                        filled_scaler = tansformation_instance.get_filled_scaler()\n",
    "                        filled_encoder = tansformation_instance.get_filled_encoder()\n",
    "                        filled_scaler_y = tansformation_instance.get_filled_scaler_y()\n",
    "                        numeric_columns_names = tansformation_instance.numeric_columns\n",
    "                        categorical_columns_names = tansformation_instance.categorical_columns\n",
    "\n",
    "                    print(\"transformed_df: \", transformed_df)\n",
    "                    print(\"Scaler_Type!!!!!!!\",type(filled_scaler))\n",
    "#                     print(\"first: \", transformed_df.iloc[0,:])\n",
    "\n",
    "                    dependent_variable = target_var\n",
    "                    ## Modeling\n",
    "                    modeling_instance = Modeling(source, transformed_df, file_name, problem_type, type_column, dependent_variable)\n",
    "                    if problem_type.lower() == \"time series\":\n",
    "                        modeling_instance.update_attributes(scaled_data = transformed_df,\n",
    "                                                            scaler = filled_scaler,\n",
    "                                                            sequence_length = 5,\n",
    "                                                            test_size=0.2, lstm_units=50,\n",
    "                                                            epochs = 2,\n",
    "                                                            numeric_columns = numeric_columns_names)\n",
    "                        modeling_instance.time_series(filled_encoder, filled_scaler)\n",
    "\n",
    "                    if problem_type.lower() == \"categorical\":\n",
    "                        modeling_instance.update_attributes(X_data = transformed_df,\n",
    "                                                            Y_data = transformed_df_y,\n",
    "                                                            scaler = filled_scaler,\n",
    "                                                            label_encoder = filled_encoder,\n",
    "                                                            label_encoder_y = filled_encoder_y,\n",
    "                                                            test_size=0.2,\n",
    "                                                            n_estimators = 150,\n",
    "                                                            max_depth = 10,\n",
    "                                                            epochs = 10,\n",
    "                                                           )\n",
    "                        modeling_instance.run_modeling()\n",
    "\n",
    "\n",
    "                    if problem_type.lower() == \"numerical\":\n",
    "                        multi_linear_reg = MultiLinearRegression(\n",
    "                            source = source,\n",
    "                            X = transformed_df, \n",
    "                            y = transformed_df_y, \n",
    "                            target_variable = dependent_variable, \n",
    "                            scaler = filled_scaler, \n",
    "                            scaler_y = filled_scaler_y, \n",
    "                            label_encoder = filled_encoder , \n",
    "                            file_name = file_name, \n",
    "                            problem_type = problem_type)\n",
    "                        mse = multi_linear_reg.evaluate()\n",
    "                        print(\"MSE: \",mse)\n",
    "\n",
    "\n",
    "                    # Hamza code for automated report generation\n",
    "                    try:\n",
    "                        df = pd.read_csv(f\"Knowledge/{problem_type}/{source}/{file_name}/dataset/{file_name}\")\n",
    "                        question = 'explain it'\n",
    "                        display(df.head())\n",
    "\n",
    "                        # Sample dictionary\n",
    "                        json_path = f'Knowledge/{problem_type}/{source}/{file_name}/json/file_paths.json'\n",
    "                        with open(json_path) as f:\n",
    "                            json_file = json.load(f)\n",
    "                        print(json_file)\n",
    "\n",
    "                        report = DataScienceReport(auth_token,df, \"explain it\",json_file, problem_type, source, file_name)\n",
    "                        report.text_analysis(auth_token, df,question)\n",
    "                        report.graph_analyze(question)\n",
    "\n",
    "                        report.generate_word_document()\n",
    "                        print(\"Dynamic Report Created Successfully.\")\n",
    "\n",
    "                    except FileNotFoundError as e:\n",
    "                        print(\"FileNotFoundError:\", e)\n",
    "                    except Exception as e:\n",
    "                        print(\"An error occurred:\", e)\n",
    "                        \n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", str(e))\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a02045",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # This block ensures that the main function is only called when the script is run, not when it's imported as a module.\n",
    "    pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7578e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ff4e823",
   "metadata": {},
   "source": [
    "## Real-time analysis / CHAT BOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f937474",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-06 11:48:12.171291: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-06 11:48:12.174621: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-06 11:48:12.216216: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-06 11:48:13.116702: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a20ff2e1c6ff4307a4436bea57146137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['use_nested_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Your GPU supports bfloat16: accelerate training with bf16=True\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect\n",
      "  warnings.warn(\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0dc664090014f9dbbcdb0c59fd0ae83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "___ Visualization of first few rows of your data ___\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Time</th>\n",
       "      <th>BG</th>\n",
       "      <th>CGM</th>\n",
       "      <th>CHO</th>\n",
       "      <th>insulin</th>\n",
       "      <th>LBGI</th>\n",
       "      <th>HBGI</th>\n",
       "      <th>Risk</th>\n",
       "      <th>Patient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10/25/2023 6:00</td>\n",
       "      <td>126.013943</td>\n",
       "      <td>136.435033</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013933</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.446600</td>\n",
       "      <td>0.446600</td>\n",
       "      <td>adolescent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10/25/2023 6:05</td>\n",
       "      <td>126.589661</td>\n",
       "      <td>137.121412</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013933</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.483302</td>\n",
       "      <td>0.483302</td>\n",
       "      <td>adolescent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>10/25/2023 6:10</td>\n",
       "      <td>127.155902</td>\n",
       "      <td>138.398018</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013933</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.520644</td>\n",
       "      <td>0.520644</td>\n",
       "      <td>adolescent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10/25/2023 6:15</td>\n",
       "      <td>127.712577</td>\n",
       "      <td>140.060899</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013933</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.558542</td>\n",
       "      <td>0.558542</td>\n",
       "      <td>adolescent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>10/25/2023 6:20</td>\n",
       "      <td>128.259611</td>\n",
       "      <td>141.830932</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013933</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.596914</td>\n",
       "      <td>0.596914</td>\n",
       "      <td>adolescent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0             Time          BG         CGM  CHO   insulin  LBGI  \\\n",
       "0           0  10/25/2023 6:00  126.013943  136.435033  0.0  0.013933   0.0   \n",
       "1           1  10/25/2023 6:05  126.589661  137.121412  0.0  0.013933   0.0   \n",
       "2           2  10/25/2023 6:10  127.155902  138.398018  0.0  0.013933   0.0   \n",
       "3           3  10/25/2023 6:15  127.712577  140.060899  0.0  0.013933   0.0   \n",
       "4           4  10/25/2023 6:20  128.259611  141.830932  0.0  0.013933   0.0   \n",
       "\n",
       "       HBGI      Risk     Patient  \n",
       "0  0.446600  0.446600  adolescent  \n",
       "1  0.483302  0.483302  adolescent  \n",
       "2  0.520644  0.520644  adolescent  \n",
       "3  0.558542  0.558542  adolescent  \n",
       "4  0.596914  0.596914  adolescent  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "___ The following data is available for the file you selected please, write your query Accordingly ___ \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.renderjson a              { text-decoration: none; }\n",
       ".renderjson .disclosure    { color: grey; font-size: 125%; }\n",
       ".renderjson .syntax        { color: grey; }\n",
       ".renderjson .string        { color: #fe46a5; }\n",
       ".renderjson .number        { color: #0f9b8e; }\n",
       ".renderjson .boolean       { color: black; }\n",
       ".renderjson .key           { color: #2684ff; }\n",
       ".renderjson .keyword       { color: gray; }\n",
       ".renderjson .object.syntax { color: gray; }\n",
       ".renderjson .array.syntax  { color: gray; }</style><div id=\"5ef182ff-efa5-499f-a98f-95cff1862846\"></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<script>var renderjson=function(){var t=function(){for(var t=[];arguments.length;)t.push(n(s(Array.prototype.shift.call(arguments)),o(Array.prototype.shift.call(arguments))));return t},n=function(){for(var t=Array.prototype.shift.call(arguments),e=0;e<arguments.length;e++)arguments[e].constructor==Array?n.apply(this,[t].concat(arguments[e])):t.appendChild(arguments[e]);return t},e=function(t,n){return t.insertBefore(n,t.firstChild),t},r=function(t,n){var e=n||Object.keys(t);for(var r in e)if(Object.hasOwnProperty.call(t,e[r]))return!1;return!0},o=function(t){return document.createTextNode(t)},s=function(t){var n=document.createElement(\"span\");return t&&(n.className=t),n},l=function(t,n,e){var r=document.createElement(\"a\");return n&&(r.className=n),r.appendChild(o(t)),r.href=\"#\",r.onclick=function(t){return e(),t&&t.stopPropagation(),!1},r};function a(i,c,u,p,y){var _=u?\"\":c,f=function(r,a,i,c,u){var f,g=s(c),h=function(){f||n(g.parentNode,f=e(u(),l(y.hide,\"disclosure\",(function(){f.style.display=\"none\",g.style.display=\"inline\"})))),f.style.display=\"inline\",g.style.display=\"none\"};n(g,l(y.show,\"disclosure\",h),t(c+\" syntax\",r),l(a,null,h),t(c+\" syntax\",i));var d=n(s(),o(_.slice(0,-1)),g);return p>0&&\"string\"!=c&&h(),d};return null===i?t(null,_,\"keyword\",\"null\"):void 0===i?t(null,_,\"keyword\",\"undefined\"):\"string\"==typeof i&&i.length>y.max_string_length?f('\"',i.substr(0,y.max_string_length)+\" ...\",'\"',\"string\",(function(){return n(s(\"string\"),t(null,_,\"string\",JSON.stringify(i)))})):\"object\"!=typeof i||[Number,String,Boolean,Date].indexOf(i.constructor)>=0?t(null,_,typeof i,JSON.stringify(i)):i.constructor==Array?0==i.length?t(null,_,\"array syntax\",\"[]\"):f(\"[\",y.collapse_msg(i.length),\"]\",\"array\",(function(){for(var e=n(s(\"array\"),t(\"array syntax\",\"[\",null,\"\\n\")),r=0;r<i.length;r++)n(e,a(y.replacer.call(i,r,i[r]),c+\"    \",!1,p-1,y),r!=i.length-1?t(\"syntax\",\",\"):[],o(\"\\n\"));return n(e,t(null,c,\"array syntax\",\"]\")),e})):r(i,y.property_list)?t(null,_,\"object syntax\",\"{}\"):f(\"{\",y.collapse_msg(Object.keys(i).length),\"}\",\"object\",(function(){var e=n(s(\"object\"),t(\"object syntax\",\"{\",null,\"\\n\"));for(var r in i)var l=r;var u=y.property_list||Object.keys(i);for(var _ in y.sort_objects&&(u=u.sort()),u){(r=u[_])in i&&n(e,t(null,c+\"    \",\"key\",'\"'+r+'\"',\"object syntax\",\": \"),a(y.replacer.call(i,r,i[r]),c+\"    \",!0,p-1,y),r!=l?t(\"syntax\",\",\"):[],o(\"\\n\"))}return n(e,t(null,c,\"object syntax\",\"}\")),e}))}var i=function t(e){var r=new Object(t.options);r.replacer=\"function\"==typeof r.replacer?r.replacer:function(t,n){return n};var o=n(document.createElement(\"pre\"),a(e,\"\",!1,r.show_to_level,r));return o.className=\"renderjson\",o};return i.set_icons=function(t,n){return i.options.show=t,i.options.hide=n,i},i.set_show_to_level=function(t){return i.options.show_to_level=\"string\"==typeof t&&\"all\"===t.toLowerCase()?Number.MAX_VALUE:t,i},i.set_max_string_length=function(t){return i.options.max_string_length=\"string\"==typeof t&&\"none\"===t.toLowerCase()?Number.MAX_VALUE:t,i},i.set_sort_objects=function(t){return i.options.sort_objects=t,i},i.set_replacer=function(t){return i.options.replacer=t,i},i.set_collapse_msg=function(t){return i.options.collapse_msg=t,i},i.set_property_list=function(t){return i.options.property_list=t,i},i.set_show_by_default=function(t){return i.options.show_to_level=t?Number.MAX_VALUE:0,i},i.options={},i.set_icons(\"⊕\",\"⊖\"),i.set_show_by_default(!1),i.set_sort_objects(!1),i.set_max_string_length(\"none\"),i.set_replacer(void 0),i.set_property_list(void 0),i.set_collapse_msg((function(t){return t+\" item\"+(1==t?\"\":\"s\")})),i}(); renderjson.set_show_to_level(1); document.getElementById(\"5ef182ff-efa5-499f-a98f-95cff1862846\").appendChild(renderjson({\"Summary Statistics \": \"Knowledge/categorical/csv/107/csv/summary_statistics.csv\", \"Missing number plot before cleaning \": \"Knowledge/categorical/csv/107/graphs/mising_number_plot.png\", \"Bar charts \": \"Knowledge/categorical/csv/107/graphs/bar_chart\", \"Crosstabs of two columns \": \"Knowledge/categorical/csv/107/csv/Crosstab\", \"Cross tabulation graph of two columns \": \"Knowledge/categorical/csv/107/graphs/Cross_tabulation\", \"Chi-Square statistics \": \"Knowledge/categorical/csv/107/csv/chi_square_statistics.csv\", \"Histogram of two columns \": \"Knowledge/categorical/csv/107/graphs/histogram\", \"Probability distributions \": \"Knowledge/categorical/csv/107/graphs/probability_distributions.png\", \"Correlation Heatmap \": \"Knowledge/categorical/csv/107/graphs/heatmap.png\", \"Result of Kruskal-Wallis test for feature selection\": \"Knowledge/categorical/csv/107/csv/Kruskal-Wallis_result.csv\", \"Most correlated Features with dependent Variable \": \"Knowledge/categorical/csv/107/csv/most_correlated_features_with_Patient.csv\", \"Random Forest Classifier Trained model path \": \"Knowledge/categorical/csv/107/models/RFC_model_Patient.joblib\", \"Random Forest Classifier x scaler path \": \"Knowledge/categorical/csv/107/models/scaler_x_Patient.pkl\", \"Random Forest Classifier label y encoder  path \": \"Knowledge/categorical/csv/107/models/y_encoder_Patient.pkl\", \"Random Forest Classifier label x encoder path \": \"Knowledge/categorical/csv/107/models/x_encoder_Patient.pkl\", \"Random Forest Classifier column names path \": \"Knowledge/categorical/csv/107/models/column_names_Patient.txt\", \"Random Forest Classifier Accuracy \": \"Knowledge/categorical/csv/107/models/model_accuracy_Patient.txt\", \"Random Forest Classifier confusion matrix csv path \": \"Knowledge/categorical/csv/107/csv/confusion_matrix_csv_Patient.csv\", \"Random Forest Classifier confusion matrix graph path \": \"Knowledge/categorical/csv/107/graphs/confusion_matrix_Patient.png\"}))</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "# from deepseeklm import DeepSeekLM\n",
    "import json\n",
    "import pandas as pd\n",
    "# from Llama import LlamaInference\n",
    "#from chatbot_final_file import Chatbot\n",
    "#from DataAquisition import DataAquisition\n",
    "from databrickschatbotapi.DatascientistPipeline.realtime_analysis import RealtimeAnalysis\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import JSON\n",
    "#from Graph_description import GraphDescriptionPipeline\n",
    "import mercury as mr\n",
    "problem_type = \"categorical\"\n",
    "source = 'csv'\n",
    "auth_token = \"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\"\n",
    "file_name = \"Diabeties_iPsafzJ.csv\"\n",
    "process_id= str(107)\n",
    "\n",
    "# folder_path = \"Datasets/denormalized_datasets\"\n",
    "df=f\"Knowledge/categorical/csv/107/dataset/Diabeties_iPsafzJ.csv\"\n",
    "#df= f\"Knowledge/{problem_type}/{source}/{process_id}/dataset/{file_name}\"\n",
    "dfhead = pd.read_csv(df)\n",
    "# problem_type = \"numerical\"\n",
    "\n",
    "json_path=f\"Knowledge/categorical/csv/107/json/file_paths.json\"\n",
    "#json_path = f\"Knowledge/{problem_type}/{source}/{file_name}/json/file_paths.json\"\n",
    "with open(json_path) as f:\n",
    "    json_file = json.load(f)\n",
    "    \n",
    "analyzer = RealtimeAnalysis(auth_token ,df , file_name,process_id, problem_type, source,json_file)\n",
    "\n",
    "print(\"\\n___ Visualization of first few rows of your data ___\")\n",
    "display(dfhead.head())\n",
    "print(\" \\n___ The following data is available for the file you selected please, write your query Accordingly ___ \\n\")\n",
    "mr.JSON(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa5d2bc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write your query & Enter 'exit' to end : write me the summmary statistics of the file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=1000) and `max_length`(=500) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of all functions ['Summary statistics explainer', 'Domain Explainer', 'Crosstab of two variables', 'Chi Square Statistics for relationship', 'Bar chart visualization', 'Two variable histogram visualization', 'Probability distribution visualization', 'Missing Number plot visualization', 'Two variable cross tabulation chart', 'Heatmap of dataset', 'Confusion matrix explainer', 'inference from classification model', 'Most_Correlated_Features_explainer_csv', 'NO function matches found form list']\n",
      " query a value write me the summmary statistics of the file\n",
      "  Based on the user query provided in the prompt, the best function to select from the given list is\n",
      "\"Summary statistics explainer\". Therefore, I will execute this function to provide summary\n",
      "statistics of the file.\n",
      "\n",
      "\n",
      "The suggested function is :  Summary statistics explainer\n",
      "real_time_file :  Knowledge/categorical/csv/107/csv/summary_statistics.csv\n",
      "query realtime_analysis :  write me the summmary statistics of the file\n",
      "load INSTRUCTOR_Transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/py39/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "WARNING:chromadb.api.models.Collection:No embedding_function provided, using default embedding function: DefaultEmbeddingFunction https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_seq_length  512\n",
      "An exception occurred: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 23.69 GiB total capacity; 8.45 GiB already allocated; 7.44 MiB free; 8.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    try:\n",
    "        clear_output(wait=True)\n",
    "        query = input(\"Write your query & Enter 'exit' to end : \")\n",
    "        analyzer.run_analyzer(query)\n",
    "    except Exception as e:\n",
    "        # Code to handle the exception\n",
    "        print(f\"An exception occurred: {e}\")\n",
    "\n",
    "        \n",
    "print(\"Chat ended!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20fbf59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed89b9cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d0ab1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for regression\n",
    "{\n",
    "    \n",
    "        'BG': [135.0],\n",
    "        'CGM': [134.0],\n",
    "        'CHO': [0.0123],\n",
    "        'LBGI': [0.1],\n",
    "        'HBGI': [0.44660030],\n",
    "        'Risk': [0.446600306],\n",
    "        'Patient': ['child']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e583e90",
   "metadata": {},
   "outputs": [],
   "source": [
    " # for catagorical\n",
    "    {\n",
    "        'BG': [105.0],\n",
    "        'CGM': [104.0],\n",
    "        'CHO': [0.0123],\n",
    "        'insulin': [30.00],\n",
    "        'LBGI': [0.1],\n",
    "        'HBGI': [0.4660030],\n",
    "        'Risk': [0.4600306]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fcd20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = f\"Knowledge/{problem_type}/{source}/{file_name}/json/file_paths.json\"\n",
    "with open(json_path) as f:\n",
    "    json_file = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f310cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33570ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "value = 'Summary Statistics '\n",
    "json_file[value]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c516a79d",
   "metadata": {},
   "source": [
    "# Realtime Analysis with Dropdowns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd0d3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dropdown import DropDown\n",
    "Knowledge_base = 'Knowledge'\n",
    "dd = DropDown(Knowledge_base)\n",
    "dd.run_dropdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f3058b",
   "metadata": {},
   "source": [
    "# Code Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9768a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CodeGeneration import CodeGeneration\n",
    "code_generation = CodeGeneration(query, data)\n",
    "code_generation.run_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfff185",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b472528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('Bussiness_facility/combined_data.csv')\n",
    "\n",
    "# Convert the 'Time' column to datetime\n",
    "data['Time'] = pd.to_datetime(data['Time'], errors='coerce')\n",
    "\n",
    "# Remove the 'Time' column before setting it as the index\n",
    "data.set_index('Time', inplace=True)\n",
    "\n",
    "# Function to prepare data for LSTM\n",
    "def prepare_data_for_lstm(data, target_column, look_back=1):\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    dataset = scaler.fit_transform(data[[target_column]])\n",
    "\n",
    "    X, Y = [], []\n",
    "    for i in range(len(dataset) - look_back):\n",
    "        X.append(dataset[i:(i + look_back), 0])\n",
    "        Y.append(dataset[i + look_back, 0])\n",
    "    return np.array(X), np.array(Y), scaler\n",
    "\n",
    "# Function to create LSTM model\n",
    "def create_lstm_model(look_back):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=50, input_shape=(look_back, 1)))\n",
    "    model.add(Dense(units=1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Function to perform time series forecasting using LSTM\n",
    "def forecast_lstm(data, target_column, look_back=1, epochs=10, batch_size=1, forecast_period=10):\n",
    "    X, Y, scaler = prepare_data_for_lstm(data, target_column, look_back)\n",
    "    X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
    "\n",
    "    model = create_lstm_model(look_back)\n",
    "    model.fit(X, Y, epochs=epochs, batch_size=batch_size, verbose=2)\n",
    "\n",
    "    # Make predictions for forecast period\n",
    "    last_input = X[-1]\n",
    "    forecast_data = []\n",
    "    for _ in range(forecast_period):\n",
    "        prediction = model.predict(np.reshape(last_input, (1, look_back, 1)))\n",
    "        forecast_data.append(prediction[0][0])\n",
    "        last_input = np.append(last_input[1:], prediction[0])\n",
    "\n",
    "    # Inverse transform the forecast predictions\n",
    "    forecast_data = scaler.inverse_transform(np.array(forecast_data).reshape(-1, 1))\n",
    "    return forecast_data\n",
    "\n",
    "# Define the forecast period\n",
    "start_date = '2023-10-26 12:25:00'\n",
    "end_date = '2023-11-26 12:25:00'\n",
    "\n",
    "# Filter the data for the forecast period\n",
    "forecast_data = data[start_date:end_date]\n",
    "\n",
    "# Perform forecasting\n",
    "forecasted_values = forecast_lstm(forecast_data, target_column='BG', look_back=1, epochs=10, batch_size=1, forecast_period=100)\n",
    "\n",
    "# Plot the original data and forecasted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(data.index, data['BG'], label='Original Data')\n",
    "plt.plot(forecast_data.index[-1] + pd.date_range(start=forecast_data.index[-1], periods=len(forecasted_values), freq='D'), forecasted_values, label='Forecasted Data')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('BG')\n",
    "plt.title('Original vs Forecasted Data')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487f960a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the original data and forecasted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(data.index, data['BG'], label='Original Data')\n",
    "plt.plot(range(len(forecasted_values)), forecasted_values, label='Forecasted Data')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('BG')\n",
    "plt.title('Original vs Forecasted Data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be671c24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2a482f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a886fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74b09ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29743328",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc3c28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import load_model\n",
    "import joblib\n",
    "\n",
    "# Load the dataset for forecasting\n",
    "data = pd.read_csv('Bussiness_facility/combined_data (1).csv')\n",
    "\n",
    "# Convert the 'Time' column to datetime\n",
    "data['Time'] = pd.to_datetime(data['Time'], errors='coerce')\n",
    "\n",
    "# Remove the 'Time' column before setting it as the index\n",
    "data.set_index('Time', inplace=True)\n",
    "\n",
    "# Function to prepare data for LSTM\n",
    "def prepare_data_for_lstm(data, target_column, look_back=1, scaler=None):\n",
    "    if scaler is None:\n",
    "        raise ValueError(\"Scaler object is missing.\")\n",
    "    dataset = scaler.transform(data[[target_column]])\n",
    "\n",
    "    X, Y = [], []\n",
    "    for i in range(len(dataset) - look_back):\n",
    "        X.append(dataset[i:(i + look_back), 0])\n",
    "        Y.append(dataset[i + look_back, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('time_series_testing_model/trained_model.h5')\n",
    "\n",
    "# Load the scaler object\n",
    "scaler = joblib.load('time_series_testing_model/scaler.pkl')\n",
    "\n",
    "# Prompt user to input target column\n",
    "target_column = input(\"Enter the target column: \")\n",
    "\n",
    "# Define the prediction date\n",
    "prediction_date = pd.to_datetime('2028-11-26 12:25:00')\n",
    "\n",
    "# Filter the new data for forecasting up to the date just before the prediction date\n",
    "new_data = data[data.index < prediction_date]\n",
    "\n",
    "# Prepare data for forecasting\n",
    "X, _ = prepare_data_for_lstm(new_data, target_column=target_column, look_back=1, scaler=scaler)\n",
    "\n",
    "# Perform forecasting\n",
    "forecasted_value = model.predict(np.reshape(X[-1], (1, 1, 1)))\n",
    "\n",
    "# Inverse transform the forecasted value\n",
    "forecasted_value = scaler.inverse_transform(np.array([forecasted_value]).reshape(-1, 1))\n",
    "\n",
    "# Print the forecasted value\n",
    "print(\"Forecasted Value for\", prediction_date, \":\", forecasted_value[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef840c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "class LSTMForecaster:\n",
    "    def __init__(self, sequence_length=1):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.model = None\n",
    "    \n",
    "    def prepare_data_for_lstm(self, data):\n",
    "        X, Y = [], []\n",
    "        for i in range(len(data) - self.sequence_length):\n",
    "            X.append(data[i:(i + self.sequence_length), 0])\n",
    "            Y.append(data[i + self.sequence_length, 0])\n",
    "        return np.array(X), np.array(Y)\n",
    "    \n",
    "    def create_lstm_model(self, lstm_units=50):\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(units=lstm_units, input_shape=(self.sequence_length, 1)))\n",
    "        model.add(Dense(units=1))\n",
    "        model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "        return model \n",
    "    \n",
    "    def train_and_save_model(self, data, model_path, sequence_length=1, epochs=10, batch_size=1):\n",
    "        X, Y = self.prepare_data_for_lstm(data)\n",
    "        X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
    "\n",
    "        model = self.create_lstm_model()\n",
    "        model.fit(X, Y, epochs=epochs, batch_size=batch_size, verbose=2)\n",
    "        model.save(model_path)\n",
    "        self.model = model\n",
    "    \n",
    "    def load_model(self, model_path):\n",
    "        self.model = load_model(model_path)\n",
    "    \n",
    "    def predict(self, data, prediction_date):\n",
    "        forecasted_value = self.model.predict(np.reshape(data[-1], (1, self.sequence_length, 1)))\n",
    "        return forecasted_value[0][0]\n",
    "\n",
    "# Load the dataset for forecasting\n",
    "data = pd.read_csv('Bussiness_facility/combined_data (1).csv')\n",
    "\n",
    "# Convert the 'Time' column to datetime\n",
    "data['Time'] = pd.to_datetime(data['Time'], errors='coerce')\n",
    "\n",
    "# Remove the 'Time' column before setting it as the index\n",
    "data.set_index('Time', inplace=True)\n",
    "\n",
    "# Define the prediction date\n",
    "prediction_date = pd.to_datetime('2028-11-26 12:25:00')\n",
    "\n",
    "# Filter the new data for forecasting up to the date just before the prediction date\n",
    "new_data = data[data.index < prediction_date]\n",
    "\n",
    "# Initialize the LSTMForecaster\n",
    "lstm_forecaster = LSTMForecaster(sequence_length=1)\n",
    "\n",
    "# Load the pre-trained model\n",
    "lstm_forecaster.load_model(model_path='Knowledge/time series/xls/Combined_combined_data (1).csv/models/LSTM_model_BG.h5')\n",
    "\n",
    "# Prompt user to input target column\n",
    "target_column = input(\"Enter the target column: \")\n",
    "\n",
    "# Make predictions\n",
    "forecasted_value = lstm_forecaster.predict(new_data[[target_column]].values, prediction_date)\n",
    "\n",
    "# Print the forecasted value\n",
    "print(\"Forecasted Value for\", prediction_date, \":\", forecasted_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e51962",
   "metadata": {},
   "source": [
    "# With Scaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6835be51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.models import load_model\n",
    "import joblib\n",
    "\n",
    "class LSTMForecaster:\n",
    "    def __init__(self, sequence_length=1, scaler_path=None):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        if scaler_path:\n",
    "            self.load_scaler(scaler_path)\n",
    "    \n",
    "    def prepare_data_for_lstm(self, data):\n",
    "        X, Y = [], []\n",
    "        for i in range(len(data) - self.sequence_length):\n",
    "            X.append(data[i:(i + self.sequence_length), 0])\n",
    "            Y.append(data[i + self.sequence_length, 0])\n",
    "        return np.array(X), np.array(Y)\n",
    "    \n",
    "    def create_lstm_model(self, lstm_units=50):\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(units=lstm_units, input_shape=(self.sequence_length, 1)))\n",
    "        model.add(Dense(units=1))\n",
    "        model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "        return model \n",
    "    \n",
    "    def train_and_save_model(self, data, model_path, sequence_length=1, epochs=10, batch_size=1):\n",
    "        X, Y = self.prepare_data_for_lstm(data)\n",
    "        X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
    "\n",
    "        model = self.create_lstm_model()\n",
    "        model.fit(X, Y, epochs=epochs, batch_size=batch_size, verbose=2)\n",
    "        model.save(model_path)\n",
    "        self.model = model\n",
    "    \n",
    "    def load_model(self, model_path):\n",
    "        self.model = load_model(model_path)\n",
    "    def load_scaler(self, scaler_path):\n",
    "        print(\"Scaler path:\", scaler_path)\n",
    "        self.scaler = joblib.load(scaler_path)\n",
    "    \n",
    "    def scale_data(self, data):\n",
    "        print(\"Data:\", data)\n",
    "        print(\"Scaler type:\", type(self.scaler)) \n",
    "        if self.scaler:\n",
    "            return self.scaler.transform(data)\n",
    "        else:\n",
    "            raise ValueError(\"Scaler object is not loaded. Load scaler first.\")\n",
    "    \n",
    "    def inverse_scale(self, scaled_data):\n",
    "        if self.scaler:\n",
    "            return self.scaler.inverse_transform(scaled_data)\n",
    "        else:\n",
    "            raise ValueError(\"Scaler object is not loaded. Load scaler first.\")\n",
    "    \n",
    "    def predict(self, data, prediction_date):\n",
    "        scaled_data = self.scale_data(data)\n",
    "        forecasted_value = self.model.predict(np.reshape(scaled_data[-1], (1, self.sequence_length, 1)))\n",
    "        return self.inverse_scale(forecasted_value)[0][0]\n",
    "\n",
    "# Load the dataset for forecasting\n",
    "data = pd.read_csv('Bussiness_facility/household_power_consumption_days (2).csv')\n",
    "# Prompt user to input target column\n",
    "target_column = input(\"Enter the target column: \")\n",
    "time_series_column = input(\"Enter the time series column: \")\n",
    "# Convert the 'Time' column to datetime\n",
    "data[time_series_column] = pd.to_datetime(data[time_series_column], errors='coerce')\n",
    "\n",
    "# Remove the 'Time' column before setting it as the index\n",
    "data.set_index(time_series_column , inplace=True)\n",
    "\n",
    "# Define the prediction date\n",
    "prediction_date = pd.to_datetime('2028-11-26 12:25:00')\n",
    "\n",
    "# Filter the original data for forecasting up to the date just before the prediction date\n",
    "new_data = data[data.index < prediction_date]\n",
    "\n",
    "# Initialize the LSTMForecaster with scaler file\n",
    "lstm_forecaster = LSTMForecaster(sequence_length=1, scaler_path='Knowledge/time series/xls/Combined_household_power_consumption_days (2).csv/models/scaler_Global_active_power.pkl')\n",
    "# lstm_forecaster = LSTMForecaster(sequence_length=1, scaler_path='time_series_testing_model/scaler.pkl')\n",
    "\n",
    "# Load the pre-trained model\n",
    "lstm_forecaster.load_model(model_path='Knowledge/time series/xls/Combined_household_power_consumption_days (2).csv/models/LSTM_model_Global_active_power.h5')\n",
    "\n",
    "\n",
    "\n",
    "# Make predictions\n",
    "forecasted_value = lstm_forecaster.predict(new_data[[target_column]].values, prediction_date)\n",
    "\n",
    "# Print the forecasted value\n",
    "print(\"Forecasted Value for\", prediction_date, \":\", forecasted_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7358054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test script to load a scaler object\n",
    "import joblib\n",
    "\n",
    "# Load scaler\n",
    "scaler = joblib.load('Knowledge/time series/xls/Combined_combined_data (1).csv/models/scaler_BG.pkl')\n",
    "print(\"Loaded scaler object:\", scaler)\n",
    "print(\"Type of loaded scaler object:\", type(scaler))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dba861",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c857bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test script to load a scaler object\n",
    "import joblib\n",
    "\n",
    "# Load scaler\n",
    "scaler = joblib.load('time_series_testing_model/scaler.pkl')\n",
    "print(\"Loaded scaler object:\", scaler)\n",
    "print(\"Type of loaded scaler object:\", type(scaler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf66466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import joblib\n",
    "\n",
    "class LSTMForecaster:\n",
    "    def __init__(self, sequence_length=1):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        self.default_directory = 'Knowledge/time series/xls/Combined_household_power_consumption_days (2).csv/models/'\n",
    "        self.load_model_and_scaler()\n",
    "    \n",
    "    def load_model_and_scaler(self):\n",
    "        scaler_path = os.path.join(self.default_directory, 'scaler_Global_active_power.pkl')\n",
    "        model_path = os.path.join(self.default_directory, 'LSTM_model_Global_active_power.h5')\n",
    "        \n",
    "        print(\"Scaler path:\", scaler_path)\n",
    "        print(\"Model path:\", model_path)\n",
    "        \n",
    "        if os.path.exists(scaler_path) and os.path.exists(model_path):\n",
    "            self.scaler = joblib.load(scaler_path)\n",
    "            self.model = load_model(model_path)\n",
    "        else:\n",
    "            raise FileNotFoundError(\"Scaler or model file not found.\")\n",
    "    \n",
    "    def prepare_data_for_lstm(self, data):\n",
    "        X, Y = [], []\n",
    "        for i in range(len(data) - self.sequence_length):\n",
    "            X.append(data[i:(i + self.sequence_length), 0])\n",
    "            Y.append(data[i + self.sequence_length, 0])\n",
    "        return np.array(X), np.array(Y)\n",
    "    \n",
    "    def create_lstm_model(self, lstm_units=50):\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(units=lstm_units, input_shape=(self.sequence_length, 1)))\n",
    "        model.add(Dense(units=1))\n",
    "        model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "        return model \n",
    "    \n",
    "    def train_and_save_model(self, data, model_path, sequence_length=1, epochs=10, batch_size=1):\n",
    "        X, Y = self.prepare_data_for_lstm(data)\n",
    "        X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
    "\n",
    "        model = self.create_lstm_model()\n",
    "        model.fit(X, Y, epochs=epochs, batch_size=batch_size, verbose=2)\n",
    "        model.save(model_path)\n",
    "        self.model = model\n",
    "    \n",
    "    def scale_data(self, data):\n",
    "        if self.scaler:\n",
    "            return self.scaler.transform(data)\n",
    "        else:\n",
    "            raise ValueError(\"Scaler object is not loaded. Load scaler first.\")\n",
    "    \n",
    "    def inverse_scale(self, scaled_data):\n",
    "        if self.scaler:\n",
    "            return self.scaler.inverse_transform(scaled_data)\n",
    "        else:\n",
    "            raise ValueError(\"Scaler object is not loaded. Load scaler first.\")\n",
    "    \n",
    "    def predict(self, data, prediction_date):\n",
    "        scaled_data = self.scale_data(data)\n",
    "        forecasted_value = self.model.predict(np.reshape(scaled_data[-1], (1, self.sequence_length, 1)))\n",
    "        return self.inverse_scale(forecasted_value)[0][0]\n",
    "\n",
    "# Load the dataset for forecasting\n",
    "data = pd.read_csv('Bussiness_facility/household_power_consumption_days (2).csv')\n",
    "# Prompt user to input target column\n",
    "target_column = input(\"Enter the target column: \")\n",
    "time_series_column = input(\"Enter the time series column: \")\n",
    "# Convert the 'Time' column to datetime\n",
    "data[time_series_column] = pd.to_datetime(data[time_series_column], errors='coerce')\n",
    "\n",
    "# Remove the 'Time' column before setting it as the index\n",
    "data.set_index(time_series_column , inplace=True)\n",
    "\n",
    "# Define the prediction date\n",
    "prediction_date = pd.to_datetime('2028-11-26 12:25:00')\n",
    "\n",
    "# Filter the original data for forecasting up to the date just before the prediction date\n",
    "new_data = data[data.index < prediction_date]\n",
    "\n",
    "# Initialize the LSTMForecaster\n",
    "lstm_forecaster = LSTMForecaster(sequence_length=1)\n",
    "\n",
    "# Make predictions\n",
    "forecasted_value = lstm_forecaster.predict(new_data[[target_column]].values, prediction_date)\n",
    "\n",
    "# Print the forecasted value\n",
    "print(\"Forecasted Value for\", prediction_date, \":\", forecasted_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8249e720",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import joblib\n",
    "\n",
    "class LSTMForecaster:\n",
    "    def __init__(self, sequence_length=1):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        self.default_directory = 'Knowledge/time series/xls/Combined_household_power_consumption_days (2).csv/models/'\n",
    "        self.load_model_and_scaler()\n",
    "    \n",
    "    def load_model_and_scaler(self):\n",
    "        scaler_path = os.path.join(self.default_directory, 'scaler_Global_active_power.pkl')\n",
    "        model_path = os.path.join(self.default_directory, 'LSTM_model_Global_active_power.h5')\n",
    "        \n",
    "        if os.path.exists(scaler_path) and os.path.exists(model_path):\n",
    "            self.scaler = joblib.load(scaler_path)\n",
    "            self.model = load_model(model_path)\n",
    "        else:\n",
    "            raise FileNotFoundError(\"Scaler or model file not found.\")\n",
    "    \n",
    "    def prepare_data_for_lstm(self, data):\n",
    "        X, Y = [], []\n",
    "        for i in range(len(data) - self.sequence_length):\n",
    "            X.append(data[i:(i + self.sequence_length), 0])\n",
    "            Y.append(data[i + self.sequence_length, 0])\n",
    "        return np.array(X), np.array(Y)\n",
    "    \n",
    "    def create_lstm_model(self, lstm_units=50):\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(units=lstm_units, input_shape=(self.sequence_length, 1)))\n",
    "        model.add(Dense(units=1))\n",
    "        model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "        return model \n",
    "    \n",
    "    def train_and_save_model(self, data, model_path, sequence_length=1, epochs=10, batch_size=1):\n",
    "        X, Y = self.prepare_data_for_lstm(data)\n",
    "        X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
    "\n",
    "        model = self.create_lstm_model()\n",
    "        model.fit(X, Y, epochs=epochs, batch_size=batch_size, verbose=2)\n",
    "        model.save(model_path)\n",
    "        self.model = model\n",
    "    \n",
    "    def scale_data(self, data):\n",
    "        if self.scaler:\n",
    "            return self.scaler.transform(data)\n",
    "        else:\n",
    "            raise ValueError(\"Scaler object is not loaded. Load scaler first.\")\n",
    "    \n",
    "    def inverse_scale(self, scaled_data):\n",
    "        if self.scaler:\n",
    "            return self.scaler.inverse_transform(scaled_data)\n",
    "        else:\n",
    "            raise ValueError(\"Scaler object is not loaded. Load scaler first.\")\n",
    "    \n",
    "    def predict(self, data, prediction_date):\n",
    "        scaled_data = self.scale_data(data)\n",
    "        forecasted_value = self.model.predict(np.reshape(scaled_data[-1], (1, self.sequence_length, 1)))\n",
    "        return self.inverse_scale(forecasted_value)[0][0]\n",
    "\n",
    "# Load the dataset for forecasting\n",
    "data = pd.read_csv('Bussiness_facility/household_power_consumption_days (2).csv')\n",
    "# Prompt user to input target column\n",
    "target_column = input(\"Enter the target column: \")\n",
    "time_series_column = input(\"Enter the time series column: \")\n",
    "# Convert the 'Time' column to datetime\n",
    "data[time_series_column] = pd.to_datetime(data[time_series_column], errors='coerce')\n",
    "\n",
    "# Remove the 'Time' column before setting it as the index\n",
    "data.set_index(time_series_column , inplace=True)\n",
    "\n",
    "# Prompt user to input the prediction date\n",
    "prediction_date_input = input(\"Enter the prediction date (YYYY-MM-DD HH:MM:SS): \")\n",
    "prediction_date = pd.to_datetime(prediction_date_input)\n",
    "\n",
    "# Filter the original data for forecasting up to the date just before the prediction date\n",
    "new_data = data[data.index < prediction_date]\n",
    "\n",
    "# Initialize the LSTMForecaster\n",
    "lstm_forecaster = LSTMForecaster(sequence_length=1)\n",
    "\n",
    "# Make predictions\n",
    "forecasted_value = lstm_forecaster.predict(new_data[[target_column]].values, prediction_date)\n",
    "\n",
    "# Print the forecasted value\n",
    "print(\"Forecasted Value for\", prediction_date, \":\", forecasted_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59241cde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
