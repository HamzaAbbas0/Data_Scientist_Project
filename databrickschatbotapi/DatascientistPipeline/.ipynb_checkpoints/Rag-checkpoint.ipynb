{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c61c125",
   "metadata": {},
   "source": [
    "# Csv File Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3b77ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No embedding_function provided, using default embedding function: DefaultEmbeddingFunction https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "# import\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "# load the document and split it into chunks\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "loader = CSVLoader(file_path=\"Knowledge/categorical/xls/Combined_Combined_combined_data (1).csv/csv/summary_statistics.csv\")\n",
    "\n",
    "documents = loader.load()\n",
    "\n",
    "\n",
    "# split it into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# create the open-source embedding function\n",
    "# embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "embedding_function = HuggingFaceInstructEmbeddings(model_name='hkunlp/instructor-xl', model_kwargs={\"device\": \"cuda\"})\n",
    "\n",
    "# load it into Chroma\n",
    "db = Chroma.from_documents(docs, embedding_function)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0384eb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content=': min\\nBG: 6.601302710067708\\nCGM: 39.0\\nCHO: 0.0\\ninsulin: 0.006575\\nLBGI: 0.0\\nHBGI: 0.0\\nRisk: 4.675104650573332e-09' metadata={'source': 'Knowledge/categorical/xls/Combined_Combined_combined_data (1).csv/csv/summary_statistics.csv', 'row': 3}\n"
     ]
    }
   ],
   "source": [
    "# query it\n",
    "query = \"give me the min value of BG column \"\n",
    "docs = db.similarity_search(query)\n",
    "\n",
    "# print results\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d0674b",
   "metadata": {},
   "source": [
    "# PDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8996e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"Bussiness_facility/longnet_paper.pdf\")\n",
    "\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a6102f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9199ad5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000 ,chunk_overlap =200)\n",
    "documents=text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc602596",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13292c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "instructor_embedding = HuggingFaceInstructEmbeddings(model_name = 'hkunlp/instructor-xl',model_kwargs={\"device\":\"cuda\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c971a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "db_instruct =FAISS.from_documents(documents, instructor_embedding)\n",
    "with open(\"fiass_database_embeddig.pkl\",'wb')as f:\n",
    "    pickle.dump(db_instruct,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e175bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"fiass_database_embeddig.pkl\",'rb') as f:\n",
    "    vector_store = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af732bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea9228b",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c83368",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vector_store.index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06b75a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "\n",
    "# Embedding the query text\n",
    "query_embedding = instructor_embedding.embed_query(query)\n",
    "\n",
    "# Perform similarity search using the embedded query\n",
    "docs = vector_store.similarity_search(query_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a6c1c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6024091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom embedding function\n",
    "def custom_embedding_function(text):\n",
    "    return instructor_embedding.embed_query(text)\n",
    "\n",
    "# Set the embedding function to the retriever\n",
    "retriever.embedding_function = custom_embedding_function\n",
    "\n",
    "# Now you can query for relevant documents\n",
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "docs = retriever.get_relevant_documents(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcb9f6a",
   "metadata": {},
   "source": [
    "# New PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ce01c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"Bussiness_facility/longnet_paper.pdf\")\n",
    "\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d047aa94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='LONGNET: Scaling Transformers to\\n1,000,000,000 Tokens\\nJiayu Dingâ™£âˆ—Shuming Maâ™ âˆ—Li Dongâ™ Xingxing Zhangâ™ \\nShaohan Huangâ™ Wenhui Wangâ™ Nanning Zhengâ™£â€ Furu Weiâ™ â€ \\nâ™ Microsoft Research\\nâ™£Xiâ€™an Jiaotong University\\nhttps://aka.ms/GeneralAI\\nAbstract\\nScaling sequence length has become a critical demand in the era of large language\\nmodels. However, existing methods struggle with either computational complexity\\nor model expressivity, rendering the maximum sequence length restricted. To\\naddress this issue, we introduce LONG NET, a Transformer variant that can scale\\nsequence length to more than 1 billion tokens, without sacrificing the performance\\non shorter sequences. Specifically, we propose dilated attention, which expands\\nthe attentive field exponentially as the distance grows. LONG NEThas significant\\nadvantages: 1) it has a linear computation complexity and a logarithm depen-\\ndency between any two tokens in a sequence; 2) it can be served as a distributed', metadata={'source': 'Bussiness_facility/longnet_paper.pdf', 'page': 0}),\n",
       " Document(page_content='trainer for extremely long sequences; 3) its dilated attention is a drop-in replace-\\nment for standard attention, which can be seamlessly integrated with the existing\\nTransformer-based optimization. Experiments results demonstrate that LONGNET\\nyields strong performance on both long-sequence modeling and general language\\ntasks. Our work opens up new possibilities for modeling very long sequences, e.g.,\\ntreating a whole corpus or even the entire Internet as a sequence. Code is available\\nathttps://aka.ms/LongNet .\\nGPT (512)Sparse Transformer \\n(12K)Reformer (64K)Memorizing \\nTransformers (262K)RMT (1M)LongNet (1B)\\n02004006008001000\\n2017 2018 2019 2020 2021 2022 2023 2024Length Millions\\nFigure 1: Trend of Transformer sequence lengths over time.\\nâˆ—Equal contribution. â€  Corresponding author.arXiv:2307.02486v2  [cs.CL]  19 Jul 2023', metadata={'source': 'Bussiness_facility/longnet_paper.pdf', 'page': 0}),\n",
       " Document(page_content='1 Introduction\\nRecent years have witnessed a trend toward scaling neural networks [ BMR+20,KMH+20,ZKHB22 ,\\nCND+22,DDM+23]. The depth is primarily scaled up for exponential expressivity, producing\\nmany powerful deep networks [ HZRS16 ,HCB+19,WMD+22]. Then, the sparse MoE mod-\\nels [LLX+21,FZS21 ,ZBK+22] and model parallelism approaches [ SPP+19,KCL+22] efficiently\\nenlarge the hidden dimension. Sequence length, as the last atomic dimension of the neural net-\\nwork, is desirable to be unlimited . Breaking the limitation of sequence length introduces significant\\nadvantages. First, it provides large memory and receptive field for models, which is practical for them\\nto interact with human and the world. Second, a longer context contains more complex causality\\nand reasoning paths that models can exploit in training data. In contrast, short dependency has more\\nspurious correlations, which is harmful to generalization. Third, it enables to explore the limits of', metadata={'source': 'Bussiness_facility/longnet_paper.pdf', 'page': 1}),\n",
       " Document(page_content='in-context learning, which has the potential to be a paradigm shift for many-shot learning, as an\\nextremely long context may help the models alleviate catastrophic forgetting.\\nThe major challenge of scaling up sequence length is striking the right balance between the\\ncomputational complexity and the model expressivity. RNN-style models are primarily imple-\\nmented to increase the length. However, its sequential nature limits the parallelization dur-\\ning training, which is essential in long-sequence modeling. More recently, state space mod-\\nels [GGR22 ,SWL23 ,FDS+23,PMN+23] are appealing to sequence modeling. It can operate\\nas a CNN during training, and transform to an efficient RNN at test time. While they perform\\nwell at long-range benchmarks [ TDA+21], their performance on regular lengths is not as good as\\nTransformers, limited mainly by the model expressivity [FPB+23].\\nAnother strand of scaling the sequence length is to decrease the complexity of Transformers, i.e.,', metadata={'source': 'Bussiness_facility/longnet_paper.pdf', 'page': 1}),\n",
       " Document(page_content='the quadratic complexity of self-attention. Implementing sliding windows or convolution modules\\nover the attention is a straightforward way to make the complexity nearly linear. Nevertheless, this\\nsacrifices the ability to recall the early tokens, forgetting the prompts at the very beginning of the\\nsequence. Sparse attention reduces the computation by sparsifying the attention matrix, preserving the\\npossibility of recalling long-distant information. For example, [ CGRS19 ] obtains O(Nâˆš\\nNd)time\\ncomplexity with a fixed sparse pattern. Besides the heuristic patterns [ ZGD+20,BPC20 ], the learnable\\npatterns prove to be useful for sparse attention [ KKL20 ,ALdJ+23]. There are also some other effi-\\ncient Transformer-based variants, including low-rank attention [ WLK+20,WCL+20], kernel-based\\nmethods [ KVPF20 ,CLD+21,QHS+22], downsampling approaches [ LLK+19,JGB+21,MKW+21],\\nrecurrent models [ DYY+19,BKB23 ], and retrieval-based methods [ WRHS22 ,WDC+23]. Yet, none', metadata={'source': 'Bussiness_facility/longnet_paper.pdf', 'page': 1})]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "text_splitter.split_documents(data)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d496bc3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='LONGNET: Scaling Transformers to\\n1,000,000,000 Tokens\\nJiayu Dingâ™£âˆ—Shuming Maâ™ âˆ—Li Dongâ™ Xingxing Zhangâ™ \\nShaohan Huangâ™ Wenhui Wangâ™ Nanning Zhengâ™£â€ Furu Weiâ™ â€ \\nâ™ Microsoft Research\\nâ™£Xiâ€™an Jiaotong University\\nhttps://aka.ms/GeneralAI\\nAbstract\\nScaling sequence length has become a critical demand in the era of large language\\nmodels. However, existing methods struggle with either computational complexity\\nor model expressivity, rendering the maximum sequence length restricted. To\\naddress this issue, we introduce LONG NET, a Transformer variant that can scale\\nsequence length to more than 1 billion tokens, without sacrificing the performance\\non shorter sequences. Specifically, we propose dilated attention, which expands\\nthe attentive field exponentially as the distance grows. LONG NEThas significant\\nadvantages: 1) it has a linear computation complexity and a logarithm depen-\\ndency between any two tokens in a sequence; 2) it can be served as a distributed', metadata={'source': 'Bussiness_facility/longnet_paper.pdf', 'page': 0}),\n",
       " Document(page_content='trainer for extremely long sequences; 3) its dilated attention is a drop-in replace-\\nment for standard attention, which can be seamlessly integrated with the existing\\nTransformer-based optimization. Experiments results demonstrate that LONGNET\\nyields strong performance on both long-sequence modeling and general language\\ntasks. Our work opens up new possibilities for modeling very long sequences, e.g.,\\ntreating a whole corpus or even the entire Internet as a sequence. Code is available\\nathttps://aka.ms/LongNet .\\nGPT (512)Sparse Transformer \\n(12K)Reformer (64K)Memorizing \\nTransformers (262K)RMT (1M)LongNet (1B)\\n02004006008001000\\n2017 2018 2019 2020 2021 2022 2023 2024Length Millions\\nFigure 1: Trend of Transformer sequence lengths over time.\\nâˆ—Equal contribution. â€  Corresponding author.arXiv:2307.02486v2  [cs.CL]  19 Jul 2023', metadata={'source': 'Bussiness_facility/longnet_paper.pdf', 'page': 0}),\n",
       " Document(page_content='1 Introduction\\nRecent years have witnessed a trend toward scaling neural networks [ BMR+20,KMH+20,ZKHB22 ,\\nCND+22,DDM+23]. The depth is primarily scaled up for exponential expressivity, producing\\nmany powerful deep networks [ HZRS16 ,HCB+19,WMD+22]. Then, the sparse MoE mod-\\nels [LLX+21,FZS21 ,ZBK+22] and model parallelism approaches [ SPP+19,KCL+22] efficiently\\nenlarge the hidden dimension. Sequence length, as the last atomic dimension of the neural net-\\nwork, is desirable to be unlimited . Breaking the limitation of sequence length introduces significant\\nadvantages. First, it provides large memory and receptive field for models, which is practical for them\\nto interact with human and the world. Second, a longer context contains more complex causality\\nand reasoning paths that models can exploit in training data. In contrast, short dependency has more\\nspurious correlations, which is harmful to generalization. Third, it enables to explore the limits of', metadata={'source': 'Bussiness_facility/longnet_paper.pdf', 'page': 1}),\n",
       " Document(page_content='in-context learning, which has the potential to be a paradigm shift for many-shot learning, as an\\nextremely long context may help the models alleviate catastrophic forgetting.\\nThe major challenge of scaling up sequence length is striking the right balance between the\\ncomputational complexity and the model expressivity. RNN-style models are primarily imple-\\nmented to increase the length. However, its sequential nature limits the parallelization dur-\\ning training, which is essential in long-sequence modeling. More recently, state space mod-\\nels [GGR22 ,SWL23 ,FDS+23,PMN+23] are appealing to sequence modeling. It can operate\\nas a CNN during training, and transform to an efficient RNN at test time. While they perform\\nwell at long-range benchmarks [ TDA+21], their performance on regular lengths is not as good as\\nTransformers, limited mainly by the model expressivity [FPB+23].\\nAnother strand of scaling the sequence length is to decrease the complexity of Transformers, i.e.,', metadata={'source': 'Bussiness_facility/longnet_paper.pdf', 'page': 1}),\n",
       " Document(page_content='the quadratic complexity of self-attention. Implementing sliding windows or convolution modules\\nover the attention is a straightforward way to make the complexity nearly linear. Nevertheless, this\\nsacrifices the ability to recall the early tokens, forgetting the prompts at the very beginning of the\\nsequence. Sparse attention reduces the computation by sparsifying the attention matrix, preserving the\\npossibility of recalling long-distant information. For example, [ CGRS19 ] obtains O(Nâˆš\\nNd)time\\ncomplexity with a fixed sparse pattern. Besides the heuristic patterns [ ZGD+20,BPC20 ], the learnable\\npatterns prove to be useful for sparse attention [ KKL20 ,ALdJ+23]. There are also some other effi-\\ncient Transformer-based variants, including low-rank attention [ WLK+20,WCL+20], kernel-based\\nmethods [ KVPF20 ,CLD+21,QHS+22], downsampling approaches [ LLK+19,JGB+21,MKW+21],\\nrecurrent models [ DYY+19,BKB23 ], and retrieval-based methods [ WRHS22 ,WDC+23]. Yet, none', metadata={'source': 'Bussiness_facility/longnet_paper.pdf', 'page': 1}),\n",
       " Document(page_content='has been scaled to 1 billion tokens (see Figure 1).\\nMethod Computation Complexity\\nRecurrent O(Nd2)\\nVanilla Attention O(N2d)\\nSparse Attention O(Nâˆš\\nNd)\\nDilated Attention (This Work) O(Nd)\\nTable 1: Comparison of computation complexity among different methods. Nis the sequence length\\nanddis the hidden dimension.\\nIn this work, we successfully scale the sequence length to 1 billion tokens . Our solution is\\nLONG NET, which replaces the attention of vanilla Transformers with a novel component named\\ndilated attention. The general design principle is - attention allocation decreases exponentially as\\nthe distance between tokens grows . We prove that it obtains a linear computation complexity and a\\nlogarithm dependency between tokens. This deals with the contradiction between limited attention\\nresources and the accessibility to every token. In the implementation, LONGNETcan be transformed\\ninto a dense Transformer, which seamlessly supports the off-the-shelf optimization for Transformers', metadata={'source': 'Bussiness_facility/longnet_paper.pdf', 'page': 1}),\n",
       " Document(page_content='(e.g., kernel fusion, quantization, and distributed training). Taking advantage of the linear complexity,\\nLONGNETcan parallelize the training across nodes, breaking the constraint of both computation and\\nmemory with a distributed algorithm. This allows us to efficiently scale up the sequence length to 1B\\n2', metadata={'source': 'Bussiness_facility/longnet_paper.pdf', 'page': 1}),\n",
       " Document(page_content='tokens with nearly constant runtime (see Figure 5), while vanilla Transformer suffers from quadratic\\ncomplexity.\\n2 L ONGNET\\n2.1 Preliminary\\nThe core of Transformers [ VSP+17] is self-attention, which maps a query and a set of keys and values\\nto output. Given the inputs Q, K, VâˆˆRNÃ—d, it computes the outputs Owith\\nO=softmax(QKT)V (1)\\nSelf-attention struggles with long sequences, due to its quadratic dependency on the sequence length.\\nOne query would attend to all keys and values, leading to computational inefficiencies.\\nSparse attention alleviates this issue by restricting the queryâ€™s access to a subset of keys and values.\\nThe key of sparse attention is the sparse attention pattern Sâˆˆ{0,1}NÃ—N, which determines specific\\nkeys and values that the query Qcan attend to.\\nO=softmax(QKTâŠ™ 1S)V (2)\\nFor example, the fixed pattern of sparse Transformer [ CGRS19 ] is composed of a local pattern and a', metadata={'source': 'Bussiness_facility/longnet_paper.pdf', 'page': 2}),\n",
       " Document(page_content='strided pattern. The sequence is divided into blocks of length l. The local pattern allows one query to\\nattend to tokens within the same block, while strided pattern allows one query to attend to the last c\\ntokens of each block. Formally, the local pattern S(1)\\ni={jâˆ£ âŒŠj/lâŒ‹=âŒŠi/lâŒ‹}, and the strided pattern\\nS(2)\\ni={jâˆ£jmodlâˆˆ{t, t+1, ..., l}}.\\n2.2 Dilated Attention\\nFigure 2 illustrates the overview of dilated attention. Dilated attention splits the input ( Q,K,V) into\\nsegments {(ÌƒQi,ÌƒKi,ÌƒVi)}N\\nwequally with a segment length w. Each segment is then sparsified along\\nthe sequence dimension by selecting the rows with an interval r. The computation can be written as:\\nÌƒQi=[Qiw, Qiw+r, Qiw+2r, ..., Q (i+1)wâˆ’1] (3)\\nÌƒKi=[Kiw, Kiw+r, Kiw+2r, ..., K (i+1)wâˆ’1] (4)\\nÌƒVi=[Viw, Viw+r, Viw+2r, ..., V(i+1)wâˆ’1] (5)\\nThe sparsified segments {(ÌƒQi,ÌƒKi,ÌƒVi)}N\\nware fed into the attention in parallel, after which are\\nscattered and concatenated as the output O:\\nÌƒOi=softmax(ÌƒQiÌƒKT\\ni)ÌƒVi (6)', metadata={'source': 'Bussiness_facility/longnet_paper.pdf', 'page': 2}),\n",
       " Document(page_content='i)ÌƒVi (6)\\nË†Oi={ÌƒOi,jâˆ£jmodr=0; 0âˆ£jmodrâ‰ 0} (7)\\nO=[Ë†O0,Ë†O1, ...,Ë†ON\\nwâˆ’1] (8)\\nIn the implementation, the dilated attention can be transformed into dense attention between a\\ngathering operation over the input (Q, K, V)and a scattering operation over the output ÌƒOi, so it can\\ndirectly reuse any optimization for vanilla attention (e.g., flash attention [ DFE+22]). Dilated attention\\ncan significantly reduce the computation cost by a factor ofN\\nwr2over the vanilla attention.\\n3', metadata={'source': 'Bussiness_facility/longnet_paper.pdf', 'page': 2}),\n",
       " Document(page_content='Segment Length: 4\\nDilated Rate: 1Segment Length: 16\\nDilated Rate: 4Segment Length: 8 \\nDilated Rate: 2Figure 2: Building blocks of dilated attention used in LONG NET. It consists of a series of attention\\npatterns for modeling both short-range and long-range dependency. The number of attention patterns\\ncan be extended according to the sequence length.\\nIn practice, the segment size wtrades the globality of attention for efficiency, while the dilation\\nwith a size rreduces the computation cost by approximating the attention matrix. To capture both\\nlong-range and short-range information efficiently, we implement a mixture of dilated attentions with\\ndifferent segment sizes and dilation rates {ri, wi}k:\\nO=k\\nâˆ‘\\ni=1Î±iOâˆ£ri,wi(9)\\nÎ±i=si\\nâˆ‘jsj(10)\\nwhere sidenotes the denominator of the attention softmax for Oâˆ£ri,wi. Note that the computations\\nfor{Oâˆ£ri,wi}kare in parallel because there is no computation dependency among them. Experiments', metadata={'source': 'Bussiness_facility/longnet_paper.pdf', 'page': 3}),\n",
       " Document(page_content='show that dynamic weights calculated by the denominator of the attention softmax are better than\\nlearnable fixed weights. For a query attends to keys in different dilated attentions, our method to mix\\ndilated attentions is equivalent to gather keys in different parts and calculate softmax together.\\nIntuitively, the local attention should be precisely computed, while the global attention can be\\napproximate. Therefore, we set a larger wiwith a bigger ri. Moreover, we gradually increase the wi\\nfor each attention until it reaches the maximum length Nor the number of attention patterns k:\\nw={w0, w1, w2, ..., N}k(wi<wi+1<N) (11)\\n4', metadata={'source': 'Bussiness_facility/longnet_paper.pdf', 'page': 3}),\n",
       " Document(page_content='Segment Length: 8\\nDilated Rate: 2\\nHeads: 41st head 2nd head 3rd head 4th headFigure 3: Dilated attention with multiple heads. The attention patterns differ among heads by shifting\\nthe position successively.\\nr={1, r1, r2, ..., r k}k(1<ri<ri+1) (12)\\nIn practice, we set wandrto geometric sequences for an exponential attentive field.\\n2.3 Multi-Head Dilated Attention\\nAs shown in Figure 3, we differ in the computation among different heads by sparsifying different\\nparts of the query-key-value pairs. Specifically, for the j-th head, we have an offset sj=jmodr\\nwhen selecting the (Q, K, V):\\nÌƒQi=[Qiw+sj, Qiw+sj+r, Qiw+sj+2r, ..., Q (i+1)w+sjâˆ’1] (13)\\nÌƒKi=[Kiw+sj, Kiw+sj+r, Kiw+sj+2r, ..., K (i+1)w+sjâˆ’1] (14)\\nÌƒVi=[Viw+sj, Viw+sj+r, Viw+sj+2r, ..., V(i+1)w+sjâˆ’1] (15)\\nFollowing the vanilla multi-head attention, the outputs of different heads are concatenated into a final\\noutput. The rest of the computation remains the same as the single-head counterpart in Section 2.2.', metadata={'source': 'Bussiness_facility/longnet_paper.pdf', 'page': 4}),\n",
       " Document(page_content='2.4 Computational Complexity and Token Dependency\\nGiven dilated attention with a segment size and dilation rate of (r, w), each query-key-value pair is\\nsparsified from (Q, K, V)âˆˆRNÃ—dto(Q, K, V)âˆˆRw\\nrÃ—d, so the flops of the attention computation\\nare estimated as:\\nFLOPs=2N\\nw(w\\nr)2d=2Nwd\\nr2(16)\\nWe further extend it to dilated attention with multiple segment sizes and dilation rates. The flops can\\nbe written as:\\nFLOPs=2Ndk\\nâˆ‘\\ni=1wi\\nr2\\ni(17)\\nWith the segment sizes and dilation rates in Equation (11) and Equation (12), the flops are given by\\nFLOPs=2w0Ndkâˆ’1\\nâˆ‘\\ni=01\\nÎ±iâ‰¤2Î±\\nÎ±âˆ’1w0Nd(Î±>1) (18)\\nwhere w0is a predefined constant and Î±is the common ratio for geometric sequences wandr.\\nTherefore, the computation complexity of dilated attention is approximate to O(Nd).\\n5', metadata={'source': 'Bussiness_facility/longnet_paper.pdf', 'page': 4}),\n",
       " Document(page_content='ð‘‹ð‘‹1 ð‘‹2ð‘„1 ð¾1 ð‘‰1 ð¾2 ð‘‰2 ð‘„2à·¨ð‘„1à·©ð¾1à·¨ð‘‰1à·©ð¾2à·¨ð‘‰2à·¨ð‘„2à·©ð¾ à·¨ð‘‰\\nSplitProjectSparsifyAll gatherð‘‚1 ð‘‚2GPU 1 GPU 2Figure 4: Distributed training of LONG NETon two GPU devices. It parallelizes the training by\\npartitioning the sequence dimension. The computation and communication costs are nearly constant\\nas the number of devices grows.\\nMoreover, the information of each tokens can be propagated to a maximum distance of D:\\nD=lâˆ’1\\nâˆ‘\\ni=0wi=w0lâˆ’1\\nâˆ‘\\ni=0Î±iâ‰ˆw0\\nÎ±âˆ’1Î±l(19)\\nwhere lis the length of the propagated path. Therefore, the maximum path length of a sequence with\\nNtokens can be estimated as:\\nLâ‰ˆlogÎ±N(Î±âˆ’1)\\nw0(Î±>1) (20)\\nThis proves that the token dependency is approximate to O(logN).\\n3 L ONGNETas a Distributed Trainer: Scaling up to 1B Tokens\\nAlthough the computation complexity of dilated attention has been greatly reduced to O(Nd), it\\nis infeasible to scale the sequence length to the million level on a single GPU device due to the', metadata={'source': 'Bussiness_facility/longnet_paper.pdf', 'page': 5}),\n",
       " Document(page_content='computation and memory constraints. There are some distributed training algorithms for large-scale\\nmodel training, such as model parallelism [ SPP+19], sequence parallelism [ LXLY21 ,KCL+22], and\\npipeline parallelism [ HCB+19]. However, they are insufficient for LONG NETespecially when the\\nsequence dimension is extremely large.\\n3.1 Distributed Algorithm\\nWe take advantage of the linear computation complexity of LONGNETfor the distributed training of\\nsequence dimension. Without loss of generality, Figure 4 presents our distributed algorithm on two\\nGPUs, which can be further scaled to an arbitrary number of devices. We start by splitting the input\\nsequence along the sequence dimension. Each sequence is put on one device separately:\\nX=[X1, X2] (21)\\nThen, they are projected into queries, keys, and values on the two devices:\\n6', metadata={'source': 'Bussiness_facility/longnet_paper.pdf', 'page': 5}),\n",
       " Document(page_content='8K16K 32K 64K128K 512K 2M 8M 32M 128M 1B\\nSequence Length10002000300040005000Runtime (ms)Dilated attention w/ FlashAttention\\nVanilla attention w/ FlashAttentionFigure 5: Runtime of our dilated attention and vanilla attention. Both are equipped with FlashAtten-\\ntion [DFE+22].\\n[Q1, K1, V1]=[WQ, WK, WV]X1,[Q2, K2, V2]=[WQ, WK, WV]X2 (22)\\nFor the segment length wiâ‰¤l(where lis the sequence length on the local device), we compute the\\nattention locally with Equation (3) to Equation (8). For the segment length wi>l, the keys and values\\nare distributed across different devices. Therefore, we collect the key-value pairs before computing\\nthe attention. We use Equation (3) to Equation (5) to sparsify the {Q, K, V}into{ÌƒQ,ÌƒK,ÌƒV}. An\\nall-gather operation is implemented to collect the key-value pairs:\\nÌƒK=[ÌƒK1,ÌƒK2],ÌƒV=[ÌƒV1,ÌƒV2] (23)\\nNote that the all-gather operation in the backward becomes a reduce-scatter operation. Different', metadata={'source': 'Bussiness_facility/longnet_paper.pdf', 'page': 6}),\n",
       " Document(page_content='from vanilla attention, both sizes of ÌƒKiandÌƒViare independent of the sequence length N, making the\\ncommunication cost constant.\\nFinally, we compute the cross-attention with the local queries ÌƒQiand the global key-value pairs\\n{ÌƒK,ÌƒV}. The formulation is written as:\\nÌƒO1=softmax(ÌƒQ1ÌƒKT)ÌƒV ,ÌƒO2=softmax(ÌƒQ2ÌƒKT)ÌƒV (24)\\nThe concatenation of the outputs across different devices becomes the final attention output:\\nÌƒO=[ÌƒO1,ÌƒO2] (25)\\nThe distributed algorithm described above is orthogonal to other parallelisms, including data paral-\\nlelism which partitions the batch dimension, model parallelism which partitions the hidden dimension,\\nand pipeline parallelism which partitions the layers.\\n3.2 Scaling up to 1B Tokens\\nWe verify the feasibility of scaling to 1B tokens with the modern distributed systems. Starting from\\n8K, we gradually scale the sequence length until the limit of GPU memory. We reduce the batch size', metadata={'source': 'Bussiness_facility/longnet_paper.pdf', 'page': 6}),\n",
       " Document(page_content='accordingly to keep the number of tokens per batch at 1 billion. Each model of different sequence\\n7', metadata={'source': 'Bussiness_facility/longnet_paper.pdf', 'page': 6}),\n",
       " Document(page_content='Model Length BatchGithub\\n2K 8K 32K\\nTransformer [VSP+17] 2K 256 4.24 5.07 11.29\\nSparse Transformer [CGRS19]8K 644.39 3.35 8.79\\nLONGNET(ours) 4.23 3.24 3.36\\nSparse Transformer [CGRS19]16K 324.85 3.73 19.77\\nLONGNET(ours) 4.27 3.26 3.31\\nSparse Transformer [CGRS19]32K 165.15 4.00 3.64\\nLONGNET(ours) 4.37 3.33 3.01\\nTable 2: Perplexity of language models for L ONG NETand the baselines.\\nlengths has up to 3 segment lengths, which are 2,048, the number of tokens per device, and the\\nsequence length. We compute the average speed in the forward propagation for 10 different runs.\\nFigure 5 reports the runtime of vanilla attention and our dilated attention. Both of them are imple-\\nmented with FlashAttention Kernel for saving memory and improving speed. It shows that dilated\\nattention can successfully scale up the sequence length with almost constant latency. By partitioning\\nthe sequence dimension, it can leverage the distributed systems to scale the sequence length to 1', metadata={'source': 'Bussiness_facility/longnet_paper.pdf', 'page': 7}),\n",
       " Document(page_content='billion tokens. In contrast, vanilla attention suffers from the quadratic dependency on the sequence\\nlength. Its latency dramatically increases as the length grows. Moreover, there is no distributed\\nalgorithm for vanilla attention to break sequence length limitation. This proves the advantage of the\\nlinear complexity as well as the distributed algorithm for L ONG NET.\\n4 Experiments on Language Modeling\\n4.1 Setup\\nWe implement LONG NETon language modeling. The backbone architecture is MAG-\\nNETO [WMH+22] with XPOS[SDP+22] relative position encoding, except that we replace the\\nstandard attention with our dilated attention. We use the base-size configuration of MAGNETO , which\\nhas a hidden dimension of 768, 12 attention heads, and 12 decoder layers. We pre-train the model\\nwith The Stack dataset [ KLA+22], a source code collection in over 300 programming languages.\\nThe data is preprocessed with the tiktoken tokenizer2with cl100k_base encoding. The models are', metadata={'source': 'Bussiness_facility/longnet_paper.pdf', 'page': 7}),\n",
       " Document(page_content='trained with a batch size of 0.5M tokens for 300K steps. More details regarding the hyperparameters\\ncan be found in the appendix. All experiments are conducted based on the torchscale [MWH+22]\\ncodebase.\\n4.2 Results\\nWe compare LONG NETwith both vanilla Transformer and sparse Transformers. The differ-\\nences among the architectures are the attention layers, while the others remain the same. We\\nscale the sequence length of these models from 2K to 32K, while reducing the batch size to\\nkeep the number of tokens per batch constant. For LONG NET, we use segment lengths of\\nw={2048,4096,8192,16384 ,32768}, and the dilated ratios are r={1,2,4,6,12}. We im-\\nplement the fixed pattern for sparse attention as in [ CGRS19 ] with multiple heads attending to distinct\\nsubblocks. The block size is set to 2048. We adjust their sparse ratios to match the computation flops\\nwith LONGNETso that the comparison is fair. The attention layers in vanilla Transformers are dense', metadata={'source': 'Bussiness_facility/longnet_paper.pdf', 'page': 7}),\n",
       " Document(page_content='and fully connected, so the computation cost is much higher. Due to the computation constraints, we\\nonly scale it up to 32K sequence length. All of our implementations of attention variants are based\\non FlashAttention3for training efficiency. We customize the flash attention kernels for both sparse\\nattention and dilated attention.\\n2https://github.com/openai/tiktoken\\n3https://github.com/HazyResearch/flash-attention/tree/main\\n8', metadata={'source': 'Bussiness_facility/longnet_paper.pdf', 'page': 7}),\n",
       " Document(page_content='1017 4Ã—10166Ã—1016\\nFLOPs46810T est PPL\\n2K\\n8K16K2K\\n8K16K32KTransformer\\nLongNetFigure 6: Test perplexity of LONGNETand dense Transformers using different sequence lengths dur-\\ning training. LONG NEToutperforms dense Transformers with a lower perplexity and a significantly\\nsmaller amount of computation.\\nTable 2 summarizes the results of these models on the Stack dataset. We use perplexity as the\\nevaluation metric. The models are tested with different sequence lengths, ranging from 2K to 32K.\\nWhen the input is longer than the maximum length that the models support, we implement block-\\nwise causal attention (BCA) [ SDP+22], a state-of-the-art extrapolation method for language model\\ninference. Besides, we remove the absolute position encoding. Primarily, the results demonstrate that\\nincreasing the sequence length during training generally leads to a better language model. Secondly,\\nthe extrapolation of sequence length in inference does not apply to the case when the length is much', metadata={'source': 'Bussiness_facility/longnet_paper.pdf', 'page': 8}),\n",
       " Document(page_content='larger than the model supports. Finally, LONG NETconsistently outperforms the baseline models,\\nproving its effectiveness in language modeling.\\n4.3 Scaling Curves of Sequence Length\\nPrevious work [ KMH+20] has shown that language models follow some scaling laws by increasing\\nparameters or training tokens. We are interested in the performance of language models when\\nthe context length is scaled up during training. We test the losses with inputs of a mixture of\\ndifferent lengths, from 1K to 32K. We use blockwise causal attention during inference to improve the\\ngeneralization of sequence lengths.\\nFigure 6 plots the scaling curves of sequence length for both vanilla Transformers and LONG NET.\\nWe estimate the amount of compute by calculating the total flops of matrix multiplication. The\\nresults show that both vanilla Transformers and LONGNETbenefit from a larger context length during\\ntraining. However, LONGNETcan scale up the context length more efficiently, achieving a lower test', metadata={'source': 'Bussiness_facility/longnet_paper.pdf', 'page': 8}),\n",
       " Document(page_content='loss with a smaller amount of computing. This demonstrates the advantage of longer training input\\nover extrapolation. In conclusion, our experiments show that LONG NETis a more efficient way to\\nscale up the context length in language models. This is because LONG NETcan learn longer-range\\ndependencies more effectively.\\n4.4 Scaling up Model Size\\nAn important property of large language models is that the loss scales as a power law with compute.\\nTo verify whether LONG NETstill follows the similar scaling law, we train a series of models with\\ndifferent model sizes, from 125 million to 2.7 billion parameters. The 2.7B model is trained with\\n9', metadata={'source': 'Bussiness_facility/longnet_paper.pdf', 'page': 8}),\n",
       " Document(page_content='101610171018\\nFLOPs1.21.41.61.82.02.22.42.6T est Loss125M\\n350M\\n760M\\n2.7BLongNet(a)\\n1K 2K 4K 8K 16K 32K\\nContext Window1.61.71.81.92.02.12.2T est LossLongNet (b)\\nFigure 7: Left: Test loss of LONG NETwith an increasing model size. The scaling curve follows\\na similar law to the vanilla Transformers. Right: Test loss of LONG NETusing different context\\nwindows. A longer context window yields better language modeling.\\n300B tokens, while the rest digest about 40B tokens. Figure 7(a) plots the scaling curve of LONGNET\\nregarding the compute. We compute the perplexity on the same test set. The amount of compute\\nis estimated by calculating the total flops of matrix multiplication during training. It proves that\\nLONGNETcan still follow the power law. This implies that the dense Transformer is not a prerequisite\\nfor scaling the language models. Additionally, the scalability and the efficiency are both obtained by\\nLONG NET.\\n4.5 Long Context Prompting', metadata={'source': 'Bussiness_facility/longnet_paper.pdf', 'page': 9}),\n",
       " Document(page_content='Prompting is an essential method to guide and provide additional information to the language models.\\nWe conduct experiments to verify whether LONG NETcan benefit from a longer context window for\\nprompting. Specifically, we reserve a piece of prefixes as the prompt and test the perplexity of its\\nsuffixes. We gradually scale the length of the prompt from 2K to 32K. For a fair comparison, we\\nkeep the suffixes the same, while increasing the length of the prefixes to the maximum lengths of the\\nmodels. The results on the test set are reported in Figure 7(b). It shows that the test loss of LONGNET\\ngradually decreases as the context window grows. This demonstrates the superiority of LONGNETin\\nfully leveraging the long context to improve the language model.\\n5 Conclusion and Future Work\\nWe present LONGNET, a Transformer variant that can scale the sequence length to 1 billion tokens and\\nbeyond, with no loss in shorter sequences. The core of LONGNETis dilated attention, which reduces', metadata={'source': 'Bussiness_facility/longnet_paper.pdf', 'page': 9}),\n",
       " Document(page_content='the computation complexity from quadratic to linear. LONGNETcan be served as a distributed trainer\\nthat parallelizes the training of a sequence across multiple GPU devices. Experiments show that\\nLONG NEThas superior performance over the strong baselines on modeling both long and short\\nsequences. In the future, we will extend LONG NETto support more tasks, e.g., multimodal large\\nlanguage modeling [ HDW+23,PWD+23], BEiT pretraining [ BDPW22 ,PDB+22,WBD+23], and\\ngenomic data modeling.\\nAcknowledgement We would like to acknowledge Yuqing Xia and Jilong Xue for the early\\nexploration of the flash attention kernel.\\nReferences\\n[ALdJ+23]Joshua Ainslie, Tao Lei, Michiel de Jong, Santiago OntaÃ±Ã³n, Siddhartha Brahma, Yury\\nZemlyanskiy, David C. Uthus, Mandy Guo, James Lee-Thorp, Yi Tay, Yun-Hsuan\\nSung, and Sumit Sanghai. CoLT5: Faster long-range transformers with conditional\\ncomputation. CoRR , abs/2303.09752, 2023.\\n10', metadata={'source': 'Bussiness_facility/longnet_paper.pdf', 'page': 9}),\n",
       " Document(page_content='[BDPW22] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. BEiT: BERT pre-training of image\\ntransformers. In International Conference on Learning Representations , 2022.\\n[BKB23] Aydar Bulatov, Yuri Kuratov, and Mikhail S. Burtsev. Scaling transformer to 1m tokens\\nand beyond with RMT. CoRR , abs/2304.11062, 2023.\\n[BMR+20]Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla\\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini\\nAgarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya\\nRamesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark\\nChen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher\\nBerner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language\\nmodels are few-shot learners. In NeurIPS 2020 , 2020.\\n[BPC20] Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document\\ntransformer. CoRR , abs/2004.05150, 2020.', metadata={'source': 'Bussiness_facility/longnet_paper.pdf', 'page': 10}),\n",
       " Document(page_content='[CGRS19] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences\\nwith sparse transformers. ArXiv , abs/1904.10509, 2019.\\n[CLD+21]Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song,\\nAndreea Gane, TamÃ¡s SarlÃ³s, Peter Hawkins, Jared Quincy Davis, Afroz Mohiud-\\ndin, Lukasz Kaiser, David Benjamin Belanger, Lucy J. Colwell, and Adrian Weller.\\nRethinking attention with performers. In 9th International Conference on Learning\\nRepresentations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021 . OpenReview.net,\\n2021.\\n[CND+22]Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Many Others, Jeff Dean, Slav\\nPetrov, and Noah Fiedel. PaLM: Scaling language modeling with Pathways. ArXiv ,\\nabs/2204.02311, 2022.\\n[DDM+23]Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek,\\nJustin Gilmer, Many Others, Xiaohua Zhai, Daniel Keysers, Jeremiah Harmsen, and Neil', metadata={'source': 'Bussiness_facility/longnet_paper.pdf', 'page': 10}),\n",
       " Document(page_content='Houlsby. Scaling vision transformers to 22 billion parameters. CoRR , abs/2302.05442,\\n2023.\\n[DFE+22]Tri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and Christopher RÃ©. Flashattention:\\nFast and memory-efficient exact attention with io-awareness. In NeurIPS , 2022.\\n[DYY+19]Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc Viet Le, and Ruslan\\nSalakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length\\ncontext. In Anna Korhonen, David R. Traum, and LluÃ­s MÃ rquez, editors, Proceedings\\nof the 57th Conference of the Association for Computational Linguistics, ACL 2019,\\nFlorence, Italy, July 28- August 2, 2019, Volume 1: Long Papers , pages 2978â€“2988.\\nAssociation for Computational Linguistics, 2019.\\n[FDS+23]Daniel Y . Fu, Tri Dao, Khaled Kamal Saab, Armin W. Thomas, Atri Rudra, and\\nChristopher RÃ©. Hungry hungry hippos: Towards language modeling with state space\\nmodels. In The Eleventh International Conference on Learning Representations, ICLR', metadata={'source': 'Bussiness_facility/longnet_paper.pdf', 'page': 10}),\n",
       " Document(page_content='2023, Kigali, Rwanda, May 1-5, 2023 . OpenReview.net, 2023.\\n[FPB+23]Mahan Fathi, Jonathan Pilault, Pierre-Luc Bacon, Christopher Pal, Orhan Firat, and\\nRoss Goroshin. Block-state transformer. CoRR , abs/2306.09539, 2023.\\n[FZS21] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion\\nparameter models with simple and efficient sparsity. CoRR , abs/2101.03961, 2021.\\n[GGR22] Albert Gu, Karan Goel, and Christopher RÃ©. Efficiently modeling long sequences\\nwith structured state spaces. In The Tenth International Conference on Learning\\nRepresentations, ICLR 2022, Virtual Event, April 25-29, 2022 . OpenReview.net, 2022.\\n11', metadata={'source': 'Bussiness_facility/longnet_paper.pdf', 'page': 10}),\n",
       " Document(page_content='[HCB+19]Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Xu\\nChen, HyoukJoong Lee, Jiquan Ngiam, Quoc V . Le, Yonghui Wu, and Zhifeng Chen.\\nGpipe: Efficient training of giant neural networks using pipeline parallelism. In NeurIPS\\n2019 , pages 103â€“112, 2019.\\n[HDW+23]Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma,\\nTengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, Kriti Aggarwal, Zewen Chi,\\nJohan Bjorck, Vishrav Chaudhary, Subhojit Som, Xia Song, and Furu Wei. Language is\\nnot all you need: Aligning perception with language models. ArXiv , abs/2302.14045,\\n2023.\\n[HZRS16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning\\nfor image recognition. In 2016 IEEE Conference on Computer Vision and Pattern\\nRecognition (CVPR) , pages 770â€“778, 2016.\\n[JGB+21]Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and\\nJoÃ£o Carreira. Perceiver: General perception with iterative attention. In Marina Meila', metadata={'source': 'Bussiness_facility/longnet_paper.pdf', 'page': 11}),\n",
       " Document(page_content='and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine\\nLearning, ICML 2021, 18-24 July 2021, Virtual Event , volume 139 of Proceedings of\\nMachine Learning Research , pages 4651â€“4664. PMLR, 2021.\\n[KCL+22]Vijay Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael Andersch,\\nMohammad Shoeybi, and Bryan Catanzaro. Reducing activation recomputation in large\\ntransformer models. CoRR , abs/2205.05198, 2022.\\n[KKL20] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient trans-\\nformer. In 8th International Conference on Learning Representations, ICLR 2020, Addis\\nAbaba, Ethiopia, April 26-30, 2020 . OpenReview.net, 2020.\\n[KLA+22]Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos MuÃ±oz\\nFerrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, Dzmitry\\nBahdanau, Leandro von Werra, and Harm de Vries. The stack: 3 TB of permissively\\nlicensed source code. CoRR , abs/2211.15533, 2022.', metadata={'source': 'Bussiness_facility/longnet_paper.pdf', 'page': 11}),\n",
       " Document(page_content='[KMH+20]Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess,\\nRewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws\\nfor neural language models. CoRR , abs/2001.08361, 2020.\\n[KVPF20] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and FranÃ§ois Fleuret. Trans-\\nformers are rnns: Fast autoregressive transformers with linear attention. In Proceedings\\nof the 37th International Conference on Machine Learning, ICML 2020, 13-18 July\\n2020, Virtual Event , volume 119 of Proceedings of Machine Learning Research , pages\\n5156â€“5165. PMLR, 2020.\\n[LJX+19]Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and\\nXifeng Yan. Enhancing the locality and breaking the memory bottleneck of transformer\\non time series forecasting. ArXiv , abs/1907.00235, 2019.\\n[LLK+19]Juho Lee, Yoonho Lee, Jungtaek Kim, Adam R. Kosiorek, Seungjin Choi, and Yee Whye\\nTeh. Set transformer: A framework for attention-based permutation-invariant neural', metadata={'source': 'Bussiness_facility/longnet_paper.pdf', 'page': 11}),\n",
       " Document(page_content='networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings\\nof the 36th International Conference on Machine Learning, ICML 2019, 9-15 June\\n2019, Long Beach, California, USA , volume 97 of Proceedings of Machine Learning\\nResearch , pages 3744â€“3753. PMLR, 2019.\\n[LLX+21]Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping\\nHuang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant\\nmodels with conditional computation and automatic sharding. In ICLR 2021 , 2021.\\n[LXLY21] Shenggui Li, Fuzhao Xue, Yongbin Li, and Yang You. Sequence parallelism: Making\\n4d parallelism possible. CoRR , abs/2105.13120, 2021.\\n12', metadata={'source': 'Bussiness_facility/longnet_paper.pdf', 'page': 11}),\n",
       " Document(page_content='[MKW+21]Xuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou, Jonathan May, Hao Ma, and\\nLuke Zettlemoyer. Luna: Linear unified nested attention. In Marcâ€™Aurelio Ranzato,\\nAlina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan,\\neditors, Advances in Neural Information Processing Systems 34: Annual Conference\\non Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021,\\nvirtual , pages 2441â€“2453, 2021.\\n[MWH+22]Shuming Ma, Hongyu Wang, Shaohan Huang, Wenhui Wang, Zewen Chi, Li Dong,\\nAlon Benhaim, Barun Patra, Vishrav Chaudhary, Xia Song, and Furu Wei. TorchScale:\\nTransformers at scale. CoRR , abs/2211.13184, 2022.\\n[PDB+22]Zhiliang Peng, Li Dong, Hangbo Bao, Qixiang Ye, and Furu Wei. BEiT v2: Masked\\nimage modeling with vector-quantized visual tokenizers. 2022.\\n[PMN+23]Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y . Fu, Tri Dao, Stephen Baccus,\\nYoshua Bengio, Stefano Ermon, and Christopher RÃ©. Hyena hierarchy: Towards larger', metadata={'source': 'Bussiness_facility/longnet_paper.pdf', 'page': 12}),\n",
       " Document(page_content='convolutional language models. CoRR , abs/2302.10866, 2023.\\n[PWD+23]Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and\\nFuru Wei. Kosmos-2: Grounding multimodal large language models to the world. ArXiv ,\\nabs/2306, 2023.\\n[QHS+22]Zhen Qin, Xiaodong Han, Weixuan Sun, Dongxu Li, Lingpeng Kong, Nick Barnes, and\\nYiran Zhong. The devil in linear transformer. In Yoav Goldberg, Zornitsa Kozareva,\\nand Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods\\nin Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates,\\nDecember 7-11, 2022 , pages 7025â€“7041. Association for Computational Linguistics,\\n2022.\\n[SDP+22]Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim,\\nVishrav Chaudhary, Xia Song, and Furu Wei. A length-extrapolatable transformer.\\nCoRR , abs/2212.10554, 2022.\\n[SPP+19]Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper,', metadata={'source': 'Bussiness_facility/longnet_paper.pdf', 'page': 12}),\n",
       " Document(page_content='and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models\\nusing model parallelism. CoRR , abs/1909.08053, 2019.\\n[SWL23] Jimmy T. H. Smith, Andrew Warrington, and Scott W. Linderman. Simplified state space\\nlayers for sequence modeling. In The Eleventh International Conference on Learning\\nRepresentations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023 . OpenReview.net, 2023.\\n[TDA+21]Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham,\\nJinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena : A\\nbenchmark for efficient transformers. In 9th International Conference on Learning\\nRepresentations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021 . OpenReview.net,\\n2021.\\n[VSP+17]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.\\nGomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS\\n2017 , pages 5998â€“6008, 2017.', metadata={'source': 'Bussiness_facility/longnet_paper.pdf', 'page': 12}),\n",
       " Document(page_content='[WBD+23]Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti\\nAggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, and Furu Wei.\\nImage as a foreign language: BEiT pretraining for vision and vision-language tasks. In\\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,\\n2023.\\n[WCL+20]Genta Indra Winata, Samuel Cahyawijaya, Zhaojiang Lin, Zihan Liu, and Pascale Fung.\\nLightweight and efficient end-to-end speech recognition using low-rank transformer.\\nIn2020 IEEE International Conference on Acoustics, Speech and Signal Processing,\\nICASSP 2020, Barcelona, Spain, May 4-8, 2020 , pages 6144â€“6148. IEEE, 2020.\\n13', metadata={'source': 'Bussiness_facility/longnet_paper.pdf', 'page': 12}),\n",
       " Document(page_content='[WDC+23]Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, and Furu\\nWei. Augmenting language models with long-term memory. CoRR , abs/2306.07174,\\n2023.\\n[WLK+20]Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer:\\nSelf-attention with linear complexity. CoRR , abs/2006.04768, 2020.\\n[WMD+22]Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, and Furu\\nWei. DeepNet: Scaling transformers to 1,000 layers. CoRR , abs/2203.00555, 2022.\\n[WMH+22]Hongyu Wang, Shuming Ma, Shaohan Huang, Li Dong, Wenhui Wang, Zhiliang Peng,\\nYu Wu, Payal Bajaj, Saksham Singhal, Alon Benhaim, Barun Patra, Zhun Liu, Vishrav\\nChaudhary, Xia Song, and Furu Wei. Foundation transformers. CoRR , abs/2210.06423,\\n2022.\\n[WRHS22] Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, and Christian Szegedy. Memo-\\nrizing transformers. In The Tenth International Conference on Learning Representations,\\nICLR 2022, Virtual Event, April 25-29, 2022 . OpenReview.net, 2022.', metadata={'source': 'Bussiness_facility/longnet_paper.pdf', 'page': 13}),\n",
       " Document(page_content='[ZBK+22]Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam\\nShazeer, and William Fedus. Designing effective sparse expert models. CoRR ,\\nabs/2202.08906, 2022.\\n[ZGD+20]Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti,\\nSantiago OntaÃ±Ã³n, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr\\nAhmed. Big bird: Transformers for longer sequences. In Hugo Larochelle, Marcâ€™Aurelio\\nRanzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in\\nNeural Information Processing Systems 33: Annual Conference on Neural Information\\nProcessing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual , 2020.\\n[ZKHB22] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision\\ntransformers. In IEEE/CVF Conference on Computer Vision and Pattern Recognition,\\nCVPR 2022, New Orleans, LA, USA, June 18-24, 2022 , pages 1204â€“1213. IEEE, 2022.\\n14', metadata={'source': 'Bussiness_facility/longnet_paper.pdf', 'page': 13}),\n",
       " Document(page_content='A Hyperparameters\\nHyperparameters Value\\nLayers 12\\nHidden size 768\\nFFN size 3072\\nHeads 12\\nLearning rate 6e-4\\nLR scheduler Polynomial decay\\nWarm-up steps 750\\nTokens per batch 500K\\nAdam Î² (0.9, 0.98)\\nTraining steps 300K\\nGradient clipping 2.0\\nDropout 0.0\\nWeight decay 0.01\\nTable 3: Hyperparamters used for the models in Table 2.\\nParameters Layers Hidden Heads Learning Rate Batch Size\\n125M 12 768 12 6e-4 500K\\n350M 24 1024 16 6e-4 500K\\n760M 24 1536 16 6e-4 500K\\n2.7B 32 2560 32 2e-4 4M\\nTable 4: Hyperparamters used for the experiments in Figure 7(a).\\n15', metadata={'source': 'Bussiness_facility/longnet_paper.pdf', 'page': 14})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents=text_splitter.split_documents(data)\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8649dd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d2cd448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ollama()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "## Load Ollama LAMA2 LLM model\n",
    "llm=Ollama(model=\"llama2\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6b09185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No embedding_function provided, using default embedding function: DefaultEmbeddingFunction https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "embedding_function = HuggingFaceInstructEmbeddings(model_name='hkunlp/instructor-xl', model_kwargs={\"device\": \"cuda\"})\n",
    "\n",
    "# load it into Chroma\n",
    "db = Chroma.from_documents(documents, embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45d0c420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x7fe361a57100>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6552c56c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'show that dynamic weights calculated by the denominator of the attention softmax are better than\\nlearnable fixed weights. For a query attends to keys in different dilated attentions, our method to mix\\ndilated attentions is equivalent to gather keys in different parts and calculate softmax together.\\nIntuitively, the local attention should be precisely computed, while the global attention can be\\napproximate. Therefore, we set a larger wiwith a bigger ri. Moreover, we gradually increase the wi\\nfor each attention until it reaches the maximum length Nor the number of attention patterns k:\\nw={w0, w1, w2, ..., N}k(wi<wi+1<N) (11)\\n4'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query=\" O = softmax(QKT âŠ™ 1S)V \"\n",
    "result=db.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b8d65ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Design ChatPrompt Template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the following question based only on the provided context. \n",
    "Think step by step before providing a detailed answer. \n",
    "I will tip you $1000 if the user finds the answer helpful. \n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "Question: {input}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e278d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), config={'run_name': 'format_inputs'})\n",
       "| ChatPromptTemplate(input_variables=['context', 'input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'input'], template='\\nAnswer the following question based only on the provided context. \\nThink step by step before providing a detailed answer. \\nI will tip you $1000 if the user finds the answer helpful. \\n<context>\\n{context}\\n</context>\\nQuestion: {input}'))])\n",
       "| Ollama()\n",
       "| StrOutputParser(), config={'run_name': 'stuff_documents_chain'})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Chain Introduction\n",
    "## Create Stuff Docment Chain\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "document_chain=create_stuff_documents_chain(llm,prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aed252d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'HuggingFaceInstructEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x7fe361a57100>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever= db.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2487d380",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "retrieval_chain=create_retrieval_chain(retriever,document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d28b4209",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fe0c01180a0>: Failed to establish a new connection: [Errno 111] Connection refused'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/urllib3/connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kw\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout:\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/urllib3/util/connection.py:95\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m socket\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgetaddrinfo returns an empty list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 85\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sock\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/urllib3/connectionpool.py:398\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 398\u001b[0m         \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhttplib_request_kw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/urllib3/connection.py:244\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers)\u001b[0m\n\u001b[1;32m    243\u001b[0m     headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _get_default_user_agent()\n\u001b[0;32m--> 244\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHTTPConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/http/client.py:1285\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1284\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1285\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/http/client.py:1331\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1330\u001b[0m     body \u001b[38;5;241m=\u001b[39m _encode(body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1331\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/http/client.py:1280\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1280\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/http/client.py:1040\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1040\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1043\u001b[0m \n\u001b[1;32m   1044\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/http/client.py:980\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[0;32m--> 980\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/urllib3/connection.py:205\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 205\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_conn(conn)\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/urllib3/connection.py:186\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m e\n\u001b[1;32m    188\u001b[0m     )\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m conn\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x7fe0c01180a0>: Failed to establish a new connection: [Errno 111] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    785\u001b[0m     e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[0;32m--> 787\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    790\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/urllib3/util/retry.py:592\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_retry\u001b[38;5;241m.\u001b[39mis_exhausted():\n\u001b[0;32m--> 592\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause))\n\u001b[1;32m    594\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fe0c01180a0>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response\u001b[38;5;241m=\u001b[39m\u001b[43mretrieval_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlongnet paper\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/langchain_core/runnables/base.py:4511\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   4506\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4507\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4508\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4509\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4510\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 4511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4512\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4513\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4514\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4515\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/langchain_core/runnables/base.py:2499\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2497\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2498\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2499\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2500\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2501\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   2502\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2503\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2504\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2505\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2506\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2507\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/langchain_core/runnables/passthrough.py:454\u001b[0m, in \u001b[0;36mRunnableAssign.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28minput\u001b[39m: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m    451\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    452\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    453\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m--> 454\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/langchain_core/runnables/base.py:1625\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1621\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m   1622\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(var_child_runnable_config\u001b[38;5;241m.\u001b[39mset, child_config)\n\u001b[1;32m   1623\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m   1624\u001b[0m         Output,\n\u001b[0;32m-> 1625\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1627\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1628\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1629\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1630\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1631\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1632\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1633\u001b[0m     )\n\u001b[1;32m   1634\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1635\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/langchain_core/runnables/config.py:347\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    346\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 347\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/langchain_core/runnables/passthrough.py:441\u001b[0m, in \u001b[0;36mRunnableAssign._invoke\u001b[0;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_invoke\u001b[39m(\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28minput\u001b[39m: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    434\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    436\u001b[0m         \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mdict\u001b[39m\n\u001b[1;32m    437\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe input to RunnablePassthrough.assign() must be a dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    440\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m--> 441\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    446\u001b[0m     }\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/langchain_core/runnables/base.py:3144\u001b[0m, in \u001b[0;36mRunnableParallel.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   3131\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m   3132\u001b[0m         futures \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   3133\u001b[0m             executor\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[1;32m   3134\u001b[0m                 step\u001b[38;5;241m.\u001b[39minvoke,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3142\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   3143\u001b[0m         ]\n\u001b[0;32m-> 3144\u001b[0m         output \u001b[38;5;241m=\u001b[39m {key: future\u001b[38;5;241m.\u001b[39mresult() \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[1;32m   3145\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3146\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/langchain_core/runnables/base.py:3144\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3131\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m   3132\u001b[0m         futures \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   3133\u001b[0m             executor\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[1;32m   3134\u001b[0m                 step\u001b[38;5;241m.\u001b[39minvoke,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3142\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   3143\u001b[0m         ]\n\u001b[0;32m-> 3144\u001b[0m         output \u001b[38;5;241m=\u001b[39m {key: \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[1;32m   3145\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3146\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/concurrent/futures/_base.py:446\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 446\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/concurrent/futures/_base.py:391\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 391\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    393\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    394\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/langchain_core/runnables/base.py:4511\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   4506\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4507\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4508\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4509\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4510\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 4511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4512\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4513\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4514\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4515\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/langchain_core/runnables/base.py:2499\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2497\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2498\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2499\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2500\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2501\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   2502\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2503\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2504\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2505\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2506\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2507\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/langchain_core/language_models/llms.py:276\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    273\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    274\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 276\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    287\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    288\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/langchain_core/language_models/llms.py:597\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    591\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    595\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    596\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 597\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/langchain_core/language_models/llms.py:767\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    753\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    754\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    755\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    765\u001b[0m         )\n\u001b[1;32m    766\u001b[0m     ]\n\u001b[0;32m--> 767\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    769\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    770\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/langchain_core/language_models/llms.py:634\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[1;32m    633\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 634\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    635\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/langchain_core/language_models/llms.py:621\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    612\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    613\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    618\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    619\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    620\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 621\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    625\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    628\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    629\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    630\u001b[0m         )\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/langchain_community/llms/ollama.py:421\u001b[0m, in \u001b[0;36mOllama._generate\u001b[0;34m(self, prompts, stop, images, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    419\u001b[0m generations \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m--> 421\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    429\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([final_chunk])\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/langchain_community/llms/ollama.py:330\u001b[0m, in \u001b[0;36m_OllamaCommon._stream_with_aggregation\u001b[0;34m(self, prompt, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_stream_with_aggregation\u001b[39m(\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    323\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    328\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m GenerationChunk:\n\u001b[1;32m    329\u001b[0m     final_chunk: Optional[GenerationChunk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 330\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_generate_stream(prompt, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m stream_resp:\n\u001b[1;32m    332\u001b[0m             chunk \u001b[38;5;241m=\u001b[39m _stream_response_to_generation_chunk(stream_resp)\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/langchain_community/llms/ollama.py:172\u001b[0m, in \u001b[0;36m_OllamaCommon._create_generate_stream\u001b[0;34m(self, prompt, stop, images, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_generate_stream\u001b[39m(\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    166\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    170\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    171\u001b[0m     payload \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m: images}\n\u001b[0;32m--> 172\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpayload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_url\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/api/generate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/langchain_community/llms/ollama.py:233\u001b[0m, in \u001b[0;36m_OllamaCommon._create_stream\u001b[0;34m(self, api_url, payload, stop, **kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m     request_payload \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    228\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: payload\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    229\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m: payload\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m, []),\n\u001b[1;32m    230\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    231\u001b[0m     }\n\u001b[0;32m--> 233\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mContent-Type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mapplication/json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_payload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m response\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/requests/adapters.py:565\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    562\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[1;32m    563\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m--> 565\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fe0c01180a0>: Failed to establish a new connection: [Errno 111] Connection refused'))"
     ]
    }
   ],
   "source": [
    "response=retrieval_chain.invoke({\"input\":\"longnet paper\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a3cb80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
