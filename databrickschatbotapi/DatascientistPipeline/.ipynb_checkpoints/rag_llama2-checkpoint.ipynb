{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed65462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from langchain import HuggingFacePipeline, PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from pdf2image import convert_from_path\n",
    "from transformers import AutoTokenizer, TextStreamer, pipeline\n",
    "from transformers import AutoModelForCausalLM\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6653df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "loader = CSVLoader(file_path=\"Knowledge/categorical/xls/Combined_Combined_combined_data (1).csv/csv/summary_statistics.csv\")\n",
    "\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b39eb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"Bussiness_facility/longnet_paper.pdf\")\n",
    "\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7944229",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceInstructEmbeddings(\n",
    "    model_name=\"hkunlp/instructor-xl\", model_kwargs={\"device\": \"cuda\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83ae443",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=64)\n",
    "texts = text_splitter.split_documents(data)\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b23b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "db = Chroma.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ac6089",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "model_basename = \"model\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True,auth_token = \"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\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.float16, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9e8cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def generate_prompt(prompt: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT) -> str:\n",
    "    return f\"\"\"\n",
    "[INST] <<SYS>>\n",
    "{system_prompt}\n",
    "<</SYS>>\n",
    "\n",
    "{prompt} [/INST]\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c3054c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#my testing\n",
    "\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\n",
    "#\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def generate_prompt(prompt: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT) -> str:\n",
    "    return f\"\"\"\n",
    "[INST] <<SYS>>\n",
    "{system_prompt}\n",
    "<</SYS>>\n",
    "\n",
    "{prompt} [/INST]\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6363732",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7bfa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=1024,\n",
    "    temperature=0.5,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.15,\n",
    "    streamer=streamer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed35eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline=text_pipeline, model_kwargs={\"temperature\": 0.6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3210bf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT=\"\"\"\n",
    "As a seasoned Data Scientist, your role is to provide a clear and concise summery statistics of the dataset \n",
    "based on user prompts,  ensuring a focus on relevant insights. \n",
    "Please don't provide the false information.\n",
    "\"\"\"\n",
    "\n",
    "template = generate_prompt(\n",
    "\"\"\"\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\",\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20d8537",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3be85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db.as_retriever(search_kwargs={\"k\": 2}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dbd5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"can you able to tell me the summary statistic\"\n",
    "result = qa_chain(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497112f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4ace7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6958d69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2656b6c",
   "metadata": {},
   "source": [
    "# With Multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797be061",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from langchain import HuggingFacePipeline, PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from pdf2image import convert_from_path\n",
    "from transformers import AutoTokenizer, TextStreamer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8561ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "loader = DirectoryLoader(\"Knowledge/categorical/xls/Combined_combined_data.csv/csv\", glob='**/*.csv', loader_cls=CSVLoader)\n",
    "\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda42567",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad66c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceInstructEmbeddings(\n",
    "    model_name=\"hkunlp/instructor-xl\", model_kwargs={\"device\": \"cuda\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361f5689",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=64)\n",
    "texts = text_splitter.split_documents(data)\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668af7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "db = Chroma.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0202f7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "model_basename = \"model\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True,auth_token = \"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\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.float16, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ec47fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#my testing\n",
    "\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\n",
    "#\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def generate_prompt(prompt: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT) -> str:\n",
    "    return f\"\"\"\n",
    "[INST] <<SYS>>\n",
    "{system_prompt}\n",
    "<</SYS>>\n",
    "\n",
    "{prompt} [/INST]\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9260fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b883b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=1024,\n",
    "    temperature=0.5,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.15,\n",
    "    streamer=streamer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd02cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline=text_pipeline, model_kwargs={\"temperature\": 0.6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6061b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT=\"\"\"\n",
    "As a seasoned Data Scientist, your role is to provide a clear and concise summery statistics of the dataset \n",
    "based on user prompts,  ensuring a focus on relevant insights. \n",
    "Please don't provide the false information.\n",
    "\"\"\"\n",
    "\n",
    "template = generate_prompt(\n",
    "\"\"\"\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\",\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae423fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c63318e",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db.as_retriever(search_kwargs={\"k\": 2}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dbfe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"can you able to tell me the summary statistic\"\n",
    "result = qa_chain(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb23452",
   "metadata": {},
   "source": [
    "# opps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0058c075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "from llama_setup import LlamaLanguageModel\n",
    "import torch\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "class Chatbot:\n",
    "    def __init__(self):\n",
    "        self.data = []\n",
    "        self.llm = LlamaLanguageModel().llm\n",
    "        self.qa_chain = None\n",
    "        self.db = None\n",
    "        self.template = self.generate_prompt(\"{context}\\nQuestion: {question}\")\n",
    "        self.prompt = PromptTemplate(template=self.template, input_variables=[\"context\", \"question\"])\n",
    "    def load_data_from_csv(self,file_path):\n",
    "        data = []\n",
    "        if os.path.exists(file_path):\n",
    "            loader = CSVLoader(file_path=file_path)\n",
    "            data = loader.load()\n",
    "        else:\n",
    "            print(f\"Warning: File not found at {file_path}\")\n",
    "        return data\n",
    "\n",
    "    def load_summary_statistics(self, file_path):\n",
    "        self.data.extend(self.load_data_from_csv(file_path))\n",
    "\n",
    "\n",
    "    def setup_qa_chain(self):\n",
    "        self.qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=self.llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=self.db.as_retriever(search_kwargs={\"k\": 2}),\n",
    "            return_source_documents=True,\n",
    "            chain_type_kwargs={\"prompt\": self.prompt},\n",
    "        )\n",
    "\n",
    "    def apply_rag(self, question):\n",
    "        result = self.qa_chain(question)\n",
    "#         return result\n",
    "\n",
    "    def generate_prompt(self, prompt: str, system_prompt: str = None) -> str:\n",
    "        system_prompt=\"\"\"\n",
    "        As a seasoned Data Scientist, your role is to provide a clear and concise summery statistics of the dataset \n",
    "        based on user prompts,  ensuring a focus on relevant insights. \n",
    "        Please don't provide the false information.\n",
    "        \"\"\"\n",
    "        return f\"\"\"\n",
    "        [INST] <<SYS>>\n",
    "        {system_prompt}\n",
    "        <</SYS>>\n",
    "        \n",
    "        {prompt} [/INST]\n",
    "        \"\"\".strip()\n",
    "\n",
    "    def setup_chroma_db(self):\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=64)\n",
    "        texts = text_splitter.split_documents(self.data)\n",
    "        self.db = Chroma.from_documents(texts, HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\", model_kwargs={\"device\": \"cuda\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ad55bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chatbot import Chatbot\n",
    "# Example usage\n",
    "chatbot = Chatbot()\n",
    "\n",
    "# Load summary statistics data\n",
    "summary_statistics_file_path = \"Knowledge/categorical/xls/Combined_Combined_combined_data (1).csv/csv/summary_statistics.csv\"\n",
    "chatbot.load_summary_statistics(summary_statistics_file_path)\n",
    "\n",
    "# Setup Chroma DB\n",
    "chatbot.setup_chroma_db()\n",
    "\n",
    "# Setup QA chain for RAG\n",
    "chatbot.setup_qa_chain()\n",
    "\n",
    "# while True:\n",
    "#     user_input = input(\"You: \")\n",
    "#     if user_input.lower() == \"exit\":\n",
    "#         print(\"Chat ended.\")\n",
    "#         break\n",
    "#     else:\n",
    "#         answer = chatbot.apply_rag(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d61248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, pipeline\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "import torch\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "\n",
    "class Chatbot:\n",
    "    def __init__(self):\n",
    "        self.data = []\n",
    "        self.llm = self.setup_language_model()\n",
    "        self.qa_chain = None\n",
    "        self.db = None\n",
    "        self.template = self.generate_prompt(\"{context}\\nQuestion: {question}\")\n",
    "        self.prompt = PromptTemplate(template=self.template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "    def load_data_from_csv(self, file_path):\n",
    "        data = []\n",
    "        if os.path.exists(file_path):\n",
    "            loader = CSVLoader(file_path=file_path)\n",
    "            data = loader.load()\n",
    "        else:\n",
    "            print(f\"Warning: File not found at {file_path}\")\n",
    "        return data\n",
    "\n",
    "    def load_summary_statistics(self, file_path):\n",
    "        self.data.extend(self.load_data_from_csv(file_path))\n",
    "\n",
    "    def setup_qa_chain(self):\n",
    "        self.qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=self.llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=self.db.as_retriever(search_kwargs={\"k\": 2}),\n",
    "            return_source_documents=True,\n",
    "            chain_type_kwargs={\"prompt\": self.prompt},\n",
    "        )\n",
    "\n",
    "    def apply_rag(self, question):\n",
    "        result = self.qa_chain(question)\n",
    "        # return result\n",
    "\n",
    "    def generate_prompt(self, prompt: str, system_prompt: str = None) -> str:\n",
    "        system_prompt = \"\"\"\n",
    "        As a seasoned Data Scientist, your role is to provide a clear and concise summary statistics of the dataset \n",
    "        based on user prompts, ensuring a focus on relevant insights. \n",
    "        Please don't provide the false information.\n",
    "        \"\"\"\n",
    "        return f\"\"\"\n",
    "        [INST] <<SYS>>\n",
    "        {system_prompt}\n",
    "        <</SYS>>\n",
    "        \n",
    "        {prompt} [/INST]\n",
    "        \"\"\".strip()\n",
    "\n",
    "    def setup_chroma_db(self):\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=64)\n",
    "        texts = text_splitter.split_documents(self.data)\n",
    "        self.db = Chroma.from_documents(texts, HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\", model_kwargs={\"device\": \"cuda\"}))\n",
    "\n",
    "    def setup_language_model(self):\n",
    "        model_name_or_path = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True, auth_token=\"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\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map='auto', torch_dtype=torch.float16)\n",
    "        streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "        text_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=1024, temperature=0.5, top_p=0.95, repetition_penalty=1.15, streamer=streamer)\n",
    "        llm = HuggingFacePipeline(pipeline=text_pipeline, model_kwargs={\"temperature\": 0.6})\n",
    "        return llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e80f83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chatbot import Chatbot\n",
    "chatbot = Chatbot()\n",
    "summary_statistics_file_path = \"Knowledge/categorical/xls/Combined_Combined_combined_data (1).csv/csv/summary_statistics.csv\"\n",
    "chatbot.load_summary_statistics(summary_statistics_file_path)\n",
    "chatbot.setup_chroma_db()\n",
    "\n",
    "# Setup QA chain for RAG\n",
    "chatbot.setup_qa_chain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e63dd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from chatbot import Chatbot\n",
    "from Llama import LlamaInference\n",
    "# Example usage\n",
    "# chatbot = Chatbot()\n",
    "llama_inference = LlamaInference(\"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\")\n",
    "# Load summary statistics data\n",
    "summary_statistics_file_path = \"Knowledge/categorical/xls/Combined_Combined_combined_data (1).csv/csv/summary_statistics.csv\"\n",
    "llama_inference.load_summary_statistics(summary_statistics_file_path)\n",
    "\n",
    "# Setup Chroma DB\n",
    "llama_inference.setup_chroma_db()\n",
    "\n",
    "# Setup QA chain for RAG\n",
    "llama_inference.setup_qa_chain()\n",
    "\n",
    "# while True:\n",
    "#     user_input = input(\"You: \")\n",
    "#     if user_input.lower() == \"exit\":\n",
    "#         print(\"Chat ended.\")\n",
    "#         break\n",
    "#     else:\n",
    "#         answer = chatbot.apply_rag(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8335e8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input =  \"tell me the bg value mean\"\n",
    "answer=chatbot.apply_rag(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a82fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"tell me the risk mAXvalue\"\n",
    "# Generate questions based on the data from the CSV file\n",
    "generated_questions = chatbot.generate_questions(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3690ef24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a1abaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d706667",
   "metadata": {},
   "source": [
    "# with llama setip_new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58eb5366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "from llama_setup_new import LlamaLanguageModel\n",
    "import torch\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "class Chatbot:\n",
    "    def __init__(self):\n",
    "        self.data = []\n",
    "        self.llm = LlamaLanguageModel().llm\n",
    "        self.qa_chain = None\n",
    "        self.db = None\n",
    "        self.template = self.generate_prompt(\"{context}\\nQuestion: {question}\")\n",
    "        self.prompt = PromptTemplate(template=self.template, input_variables=[\"context\", \"question\"])\n",
    "    def load_data_from_csv(self,file_path):\n",
    "        data = []\n",
    "        if os.path.exists(file_path):\n",
    "            loader = CSVLoader(file_path=file_path)\n",
    "            data = loader.load()\n",
    "        else:\n",
    "            print(f\"Warning: File not found at {file_path}\")\n",
    "        return data\n",
    "\n",
    "    def load_summary_statistics(self, file_path):\n",
    "        self.data.extend(self.load_data_from_csv(file_path))\n",
    "\n",
    "\n",
    "    def setup_qa_chain(self):\n",
    "        self.qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=self.llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=self.db.as_retriever(search_kwargs={\"k\": 2}),\n",
    "            return_source_documents=True,\n",
    "            chain_type_kwargs={\"prompt\": self.prompt},\n",
    "        )\n",
    "\n",
    "    def apply_rag(self, question):\n",
    "        result = self.qa_chain(question)\n",
    "#         return result\n",
    "\n",
    "    def generate_prompt(self, prompt: str, system_prompt: str = None) -> str:\n",
    "        system_prompt=\"\"\"\n",
    "        As a seasoned Data Scientist, your role is to provide a clear and concise summery statistics of the dataset \n",
    "        based on user prompts,  ensuring a focus on relevant insights. \n",
    "        Please don't provide the false information.\n",
    "        \"\"\"\n",
    "        return f\"\"\"\n",
    "        [INST] <<SYS>>\n",
    "        {system_prompt}\n",
    "        <</SYS>>\n",
    "        \n",
    "        {prompt} [/INST]\n",
    "        \"\"\".strip()\n",
    "\n",
    "    def setup_chroma_db(self):\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=64)\n",
    "        texts = text_splitter.split_documents(self.data)\n",
    "        self.db = Chroma.from_documents(texts, HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\", model_kwargs={\"device\": \"cuda\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a14ea5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "chatbot = Chatbot()\n",
    "\n",
    "# Load summary statistics data\n",
    "summary_statistics_file_path = \"Knowledge/categorical/xls/Combined_Combined_combined_data (1).csv/csv/summary_statistics.csv\"\n",
    "chatbot.load_summary_statistics(summary_statistics_file_path)\n",
    "\n",
    "# Setup Chroma DB\n",
    "chatbot.setup_chroma_db()\n",
    "\n",
    "# Setup QA chain for RAG\n",
    "chatbot.setup_qa_chain()\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        print(\"Chat ended.\")\n",
    "        break\n",
    "    else:\n",
    "        answer = chatbot.apply_rag(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af439e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Llama import LlamaInference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49468a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "loader = CSVLoader(file_path='Knowledge/categorical/xls/Combined_Diabeties.csv/csv/summary_statistics.csv')\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9590cb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_inference = LlamaInference(\"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\",data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5548425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_inference.setup_language_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec23a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_inference.setup_chroma_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81191dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_inference.setup_qa_chain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2525cd1a",
   "metadata": {},
   "source": [
    "### chatbot_Test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e350247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Chatbot class with the path to your CSV file\n",
    "from chatbot_Test import Chatbot\n",
    "csv_file_path = \"Knowledge/categorical/xls/Combined_Combined_combined_data (1).csv/csv/summary_statistics.csv\"\n",
    "chatbot = Chatbot(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d96a6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input =  \"tell me the bg value mean\"\n",
    "answer=chatbot.apply_rag(user_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be87fba0",
   "metadata": {},
   "source": [
    "## chatbot_Test file_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6ea88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Chatbot class with the path to your CSV file\n",
    "from chatbot_Test import Chatbot\n",
    "csv_file_path = \"Knowledge/categorical/xls/Combined_Combined_combined_data (1).csv/csv/summary_statistics.csv\"\n",
    "chatbot = Chatbot(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f024c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chatbot_Test_new import Chatbot\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot = Chatbot(\"Knowledge/categorical/xls/Combined_Combined_combined_data (1).csv/csv/summary_statistics.csv\")\n",
    "\n",
    "    query = \"What are the introduction of it\"\n",
    "    correlation_prompt = chatbot.generate_summary(query)\n",
    "\n",
    "    query = \"What are the max value of the Bg\"\n",
    "    result = chatbot.generate_questions(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c7b8b3",
   "metadata": {},
   "source": [
    "# chatbot_Test file_new1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e690042",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-19 17:48:40.109573: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-19 17:48:40.153825: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-19 17:48:40.997803: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "486d00e12eb048de9289f3c4e0fa26af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from chatbot_Test_new1 import Chatbot\n",
    "chatbot = Chatbot()\n",
    "\n",
    "# query = \"What are the introduction of it\"\n",
    "# correlation_prompt = chatbot.generate_summary(query)\n",
    "\n",
    "# query = \"What are the max value of the Bg\"\n",
    "# result = chatbot.generate_questions(\"Knowledge/categorical/xls/Combined_Combined_combined_data (1).csv/csv/summary_statistics.csv\",query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "732401ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/py39/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_seq_length  512\n",
      " Sure! Based on the provided data, here are some key statistics and insights that can be derived:\n",
      "\n",
      "Introduction:\n",
      "This dataset contains information on various physiological variables related to diabetes management, including blood glucose (BG) levels, carbohydrate grams (CGM), insulin dosage, and risk factors for complications. The dataset consists of 7 individuals, with varying ages and gender distributions.\n",
      "\n",
      "Minimum and Maximum Values:\n",
      "The minimum value observed in the dataset is BG = 6.601302710067708 for individual ID = 1, while the maximum value is BG = 115.95510802801054 for individual ID = 6. Similarly, the minimum CGM value is 39.0 for individual ID = 1, while the maximum value is 297.5225733305358 for individual ID = 6.\n",
      "\n",
      "Mean Values:\n",
      "The mean BG level across all individuals is BG = 111.95510802801054, while the mean CGM level is 115.45124494715583. The mean insulin dose per day is 0.015444977772516803, and the mean LBGI value is 2.975225733305358. Finally, the mean HBGI value is 0.8951001232544352.\n",
      "\n",
      "Outliers:\n",
      "One outlier was identified in the dataset, which is individual ID = 2, who has a significantly higher BG level than the other individuals (BG = 151.95510802801054). Additionally, individual ID = 3 has a much lower LBGI value than the others (LBGI = -0.475225733305358).\n",
      "\n",
      "Trends and Patterns:\n",
      "While there is no clear trend or pattern in the data regarding the relationship between age and BG levels, it is worth noting that older individuals tend to have higher BG levels overall. Additionally, there appears to be a correlation between CGM intake and BG levels, as individuals who consume more carbohydrates tend to have higher BG levels. However, further analysis and visualization may reveal additional patterns and relationships in the data.\n"
     ]
    }
   ],
   "source": [
    "query = \"Write me the introduction of the file\"\n",
    "result = chatbot.generate_summary(\"Knowledge/categorical/xls/Combined_Combined_combined_data (1).csv/csv/summary_statistics.csv\",query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "782e0ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/py39/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_seq_length  512\n",
      " As an expert data scientist, I can tell you that the topmost important features of the given file are:\n",
      "\n",
      "1. Risk: 1.0 - This feature has a value of 1.0, indicating that the file belongs to the \"Risk\" category.\n",
      "2. LBGI - This feature has a value of 0.9575936074358368, indicating that the file is related to the \"LBGI\" topic.\n",
      "\n",
      "These two features are the most important ones in the file, as they provide valuable information about the categories and topics associated with the file.\n"
     ]
    }
   ],
   "source": [
    "query = \"Write me the top most important features of this file\"\n",
    "result = chatbot.generate_corr_imp_features(\"Knowledge/time series/xls/Combined_Diabeties.csv/csv/all_correlated_features_with_Risk.csv\",query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f266d47d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
